{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Mzlij95ZZbzW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 55576,
     "status": "ok",
     "timestamp": 1761364837669,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "Mzlij95ZZbzW",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "aa2fdc6f-4d27-4661-cbc0-fc80b5964889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.37)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (0.3.79)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.4.37)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (4.15.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (2.11.10)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (2.32.4)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.4.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (1.3.1)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.0.0 (from langchain_community)\n",
      "  Downloading langchain_core-1.0.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain_community)\n",
      "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.44)\n",
      "Collecting requests<3.0.0,>=2.32.5 (from langchain_community)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.11.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.37)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain_community)\n",
      "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.11.10)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_community) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_community) (4.15.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.10.5)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.33.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.1)\n",
      "Downloading langchain_community-0.4-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-1.0.1-py3-none-any.whl (467 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.1/467.1 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-core, langchain-text-splitters, langchain-classic, langchain_community\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.79\n",
      "    Uninstalling langchain-core-0.3.79:\n",
      "      Successfully uninstalled langchain-core-0.3.79\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.11\n",
      "    Uninstalling langchain-text-splitters-0.3.11:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.1 which is incompatible.\n",
      "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.0 langchain-core-1.0.1 langchain-text-splitters-1.0.0 langchain_community-0.4 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-1.2.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.71.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.35.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.10)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Downloading chromadb-1.2.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.7/20.7 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=2390a8a7484793be943a92969190af442fce82f290f64a262e41ab0c3df9ffc4\n",
      "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, durationpy, uvloop, urllib3, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, posthog, opentelemetry-semantic-conventions, onnxruntime, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "  Attempting uninstall: opentelemetry-proto\n",
      "    Found existing installation: opentelemetry-proto 1.37.0\n",
      "    Uninstalling opentelemetry-proto-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
      "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
      "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
      "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.37.0\n",
      "    Uninstalling opentelemetry-api-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.37.0\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.37.0\n",
      "    Uninstalling opentelemetry-sdk-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.1 which is incompatible.\n",
      "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\n",
      "google-adk 1.16.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
      "google-adk 1.16.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 chromadb-1.2.1 coloredlogs-15.0.1 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain_core\n",
    "!pip install langchain_community\n",
    "!pip install chromadb\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "QcIwl0CHikE9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4490,
     "status": "ok",
     "timestamp": 1761365110806,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "QcIwl0CHikE9",
    "outputId": "0bf531f5-efb3-4723-f332-cef1c288d744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.71.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.35.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.10)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Wh5cuC0QhsK5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8629,
     "status": "ok",
     "timestamp": 1761364875709,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "Wh5cuC0QhsK5",
    "outputId": "43624091-6bf9-41a5-d24f-a60fb5dd2bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.5\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb8b8f",
   "metadata": {
    "id": "77fb8b8f"
   },
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40dfc99",
   "metadata": {
    "executionInfo": {
     "elapsed": 20415,
     "status": "ok",
     "timestamp": 1761370043013,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "e40dfc99"
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccda597",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1761370044029,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "2ccda597",
    "outputId": "e516d633-b258-4d2b-c9a4-92a0445bc642"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 0}, page_content=''), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 1}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ1. Explain the requirement of RAG when LLMs are already powerful. \\n \\nLLMs are powerful, as they are trained on large volumes of data using sophisticated \\ntechniques. However, LLMs because of knowledge cutoff (static knowledge), struggle to \\nanswer queries related to the latest events or the data not present in their training \\ncorpus.  \\n \\nRAG addresses this challenge by retrieving relevant context from external knowledge \\nsources, which allows LLMs to provide accurate responses. This is why RAG is essential \\nfor LLM-based applications that need to be accurate. Otherwise, LLMs alone might \\nprovide you answers that are incomplete or outdated. \\n \\nQ2. Is RAG still relevant in the era of long context LLMs? \\n \\nRAG is still important even with long context LLMs. This is because long-context LLMs \\nwithout RAG have three big problems: \"lost in the middle,\", high API costs, and \\nincreased latency. \\n \\nLong-context LLMs often struggle to find the most relevant  information in large \\ncontexts, which hurts the quality of generated responses. Furthermore, processing \\nlengthy sequences in each API call results in high latency and high API costs. \\n \\nRAG addresses these issues by providing the most relevant information from external \\nknowledge sources. So, you still need RAG to get accurate and cost-efficient responses, \\neven with long context LLMs. \\n \\nQ3. What are the fundamental challenges of RAG systems? \\n \\nRAG is powerful, but it has to deal with the following challenges: \\n \\nScalability: Searching and retrieving from large, dynamic knowledge sources quickly \\nand efficiently requires a lot of computing power and well-optimized indexing, which \\ncan be expensive or take a long time. \\n \\nLatency - The two-step process (retrieval then generation) can cause delays, making it \\nless suitable for real-time applications without careful optimization. \\n \\nHallucination Risk - Even with retrieval, the model might generate plausible but \\nunsupported details if the retrieved data is ambiguous or insufficient. \\n \\n1                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 2}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nBias and Noise - Retrieved content might carry biases, errors, or irrelevant noise from \\nthe web or other sources, which can propagate into the output. \\n \\nQ4. What are effective strategies to reduce latency in RAG systems? \\n \\nCaching, embedding quantization, selective query rewriting, and selective re-ranking are \\nsome of the ways to reduce RAG latency. Caching stores retrieved results or generated \\nresponses to avoid redundant computation. Embedding quantization to lower bit \\nprecision reduces memory and computational load, speeding up retrieval.  \\n \\nSelective query rewriting enhances recall and relevance by refining queries prior to \\nretrieval, primarily utilized for complex or ambiguous queries. Selective re-ranking is \\nonly used for complicated queries, which cuts down on unnecessary computation for \\nsimpler ones.  \\n \\nQ5. Explain R, A, and G in RAG.  \\n \\nRAG stands for Retrieval-Augmented Generation. The \"R\" or Retrieval, refers to the \\nprocess of searching and fetching the most relevant information from external \\nknowledge sources for the given user query.  \\n \\nThe \"A\" or Augmented, involves including the retrieved relevant context in the LLM \\nprompt having the user query and instructions so that the LLM can generate a response \\nbased on the provided context.   \\n \\nFinally, the \"G\" or Generation is the phase during which the generator LLM processes \\nthe prompt having instructions, a query, and context to generate a response that is \\ncoherent, accurate, and contextually relevant.  \\n \\nQ6. How does RAG help reduce hallucinations in LLM generated responses? \\n \\nWithout RAG, LLM answers user questions based on what it learned from the training \\ncorpus, which may not be up-to-date or complete. This could lead to hallucinated \\nresponses, which are answers that sound right but are wrong. \\n \\nRetrieval-Augmented Generation (RAG) helps cut down on hallucinations in \\nLLM-generated responses by adding an external retrieval system that pulls relevant, \\nfactual information from trusted, up-to-date external knowledge sources. \\n \\n2                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 3}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nBy combining retrieval with generation, RAG ensures that answers are more accurate, \\ncontextually relevant, and less prone to fabrications or false information, significantly \\nenhancing the reliability of the output.  \\n \\nQ7. Why is re-ranking important in the RAG pipeline after initial document \\nretrieval? \\n \\nThe top K chunks fetched by the RAG retriever may have irrelevant chunks ahead of \\nrelevant ones. Passing these results directly to the LLM hurts the quality of the answers \\nbecause LLMs mostly look at the top-ranked chunks that are given as context.  \\n \\nRe-ranking uses cross-encoder models to deeply measure the semantic relevance of  \\nquery-chunk pairs and then brings relevant chunks ahead of irrelevant chunks. This \\nreduces the noise and helps the generator LLM to generate more accurate and coherent \\nanswers.  \\n \\nQ8. What is the purpose of character overlap during chunking in a RAG \\npipeline? \\n \\nIn a RAG pipeline, chunk overlap during chunking ensures contextual continuity and \\nprevents loss of information at the boundaries of chunks. This improves the retrieval \\naccuracy and maintains coherence in the text fed to the LLM.  \\n \\nTypically, an overlap of about 10-20% of the chunk size is used to strike a balance \\nbetween preserving context and computational efficiency in RAG applications. \\n \\nQ9. What role does cosine similarity play in relevant chunk retrieval within \\na RAG pipeline? \\n \\nCosine similarity measures how similar the query embedding is to the embeddings of \\nchunks in the vector database. It finds the cosine of the angle between two vectors and \\nprovides a score that shows how closely related the query is to each chunk. Higher \\nscores mean that the chunk is more relevant.  \\n \\nThis enables the RAG system to retrieve the most relevant chunks for the query, which is \\nthen used by the generator LLM to generate accurate answers. \\n \\nQ10. Can you give examples of real-world applications where RAG systems \\nhave demonstrated value? \\n3                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 4}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\n \\nAI search engines are a great example of how RAG systems have changed the way people \\nfind information online. AI search engines give you accurate, relevant answers by \\ncombining information retrieval with generative AI.  \\n \\nFor instance, RAG-based AI search platforms like Perplexity AI improve the user \\nexperience by fetching the most recent and relevant information from large knowledge \\nbases and then giving it back in the format that the user wants. \\n \\nQ11. Explain the steps in the indexing process in a RAG pipeline. \\n \\nThere are four steps in the indexing process of a RAG pipeline: parsing, chunking, \\nencoding, and storing. The parsing step deals with extracting the document content. \\nThen, the chunking step splits the extracted content into smaller pieces called chunks.  \\n \\nThe encoding step uses an embedding model to convert chunks into dense numerical \\nvectors called embeddings. Finally, these embeddings are saved in a vector database for \\nefficient search and retrieval. \\n \\nAll these steps in the indexing process are performed offline. \\n \\nQ12. Explain the importance of chunking in RAG. \\n \\nChunking in Retrieval-Augmented Generation (RAG) is crucial because it breaks down \\nlarge texts into smaller and semantically coherent segments called chunks. Proper \\nchunking helps to find relevant information efficiently by creating focused chunks that \\nmaintain context and avoid irrelevant noise.  \\n \\nChoosing the right chunk size balances detail and context, optimizing both retrieval \\naccuracy and computational efficiency. Ineffective chunking can lead to poor retrieval \\nresults and incoherent responses, which makes it a foundational step for successful RAG \\nperformance in real-world applications. \\n \\nQ13. How do you choose the chunk size for a RAG system? \\n \\nChoosing the chunk size for a RAG system involves balancing granularity, context \\ncompleteness, and computational efficiency. Smaller chunks (e.g., 100-200 tokens) \\nallow precise retrieval but may lack sufficient context. Larger chunks (e.g., 500-1000 \\ntokens) provide more context at the cost of increased computational load and potential \\nnoise.  \\n4                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 5}, page_content=\"🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\n \\nThe optimal size depends on the use case, document structure, embedding model, and \\nthe generator (LLM) model. For example, smaller chunks are suitable for fact-based \\nqueries, and more complex queries benefit from larger ones. \\n \\nQ14. What are the potential consequences of having chunks that are too \\nlarge versus chunks that are too small? \\n \\nLarge chunks often mix different topics into one chunk and reduce the chunk's \\nrelevance. This can lead to coarse vector representations and less accurate retrieval. \\nLarge chunks can also add noise and confuse the model with irrelevant information that \\nisn't important, resulting in a less accurate answer. \\n \\nSmall chunk sizes in RAG systems can lead to fragmented context. This fragmentation \\noften leads to poor retrieval quality because information that is semantically related may \\nbe split up into chunks that are not retrieved together. Furthermore, smaller chunks \\nmean that there are more chunks in the vector database, which increases storage costs \\nand slows down the similarity search. \\n \\nQ15. Explain the retrieval process step-by-step in a RAG pipeline. \\n \\nThe retrieval process in RAG systems starts by encoding the user query, i.e., converting \\nit into a dense vector representation using an embedding model.  \\nThis vector \\nrepresentation is then used to search the vector database, which has the embeddings of \\nchunks.  Based on the similarity scores, the vector database system returns the most \\nrelevant document chunks.  \\n \\nQ16. What are the key considerations when choosing an LLM for a RAG \\nsystem? \\n \\nThe \\nkey \\nconsiderations \\nwhen \\nchoosing \\nan \\nLLM \\nfor \\na RAG system are \\nreading-comprehension \\nability, \\ncontext \\nwindow \\nsize, \\nand \\ninference \\nspeed. \\nReading-comprehension ability reflects how effectively the model processes the \\nretrieved context to generate accurate responses.  \\n \\nContext window size is crucial, as longer context models enable RAG systems to \\neffectively include more relevant chunks. However, this must be balanced against cost \\nand latency requirements.  \\n \\n5                                                          Kalyan KS (Follow on Twitter and LinkedIn)\"), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 6}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nAdditionally, inference speed, infrastructure compatibility, and licensing terms also play \\na key role in deployment decisions for real-world RAG solutions. \\n \\nQ17. How is the prompt provided to the LLM in a RAG system different from \\na standard, non-RAG prompt? \\n \\nThe prompt provided to the LLM without a RAG setup includes only the user query and \\nthe optional instructions. Here, the LLM generates the response based on its knowledge \\ngained during training. \\n \\nThe prompt provided to the LLM with the RAG setup includes the user query, \\ninstructions, and relevant context. Here, the LLM generates the response as per the \\ninstructions solely based on the provided relevant context.  \\n \\nQ18. What are the key hyperparameters in a RAG pipeline? \\n \\nChunk size, chunk overlap, embedding dimensionality, retrieval top-k, and retrieval \\nthreshold are some of the most important hyperparameters for retrieval in RAG. \\nTemperature and max output length are two important hyperparameters for RAG \\ngeneration. \\n \\nThe chunk size determines how much text is put into a segment before embedding, \\ninfluencing the context granularity retrieved. Chunk overlap repeats a set of tokens at \\nchunk boundaries, helping preserve important context across segments. Embedding \\ndimensionality is the vector size used to represent text, which affects retrieval precision \\nand database efficiency. \\n \\nRetrieval top-k sets the number of most similar chunks returned, directly impacting \\nrecall and context diversity in the response. The retrieval threshold is a similarity cutoff \\nthat filters retrieved results, ensuring only relevant chunks are selected. \\n \\nTemperature controls the randomness of generated text, balancing creativity and \\ndeterminism in model outputs. Max output length limits the number of tokens \\ngenerated, managing the verbosity and computational cost of responses. \\n \\nQ19. What are the popular frameworks to implement a RAG system? Justify \\nyour choice of framework. \\n \\nLangChain, LlamaIndex, and Haystack are the most popular frameworks for RAG \\nimplementation. LangChain is great for custom pipelines, and LlamaIndex is great for \\n6                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 7}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nefficient document indexing and retrieval. The Haystack framework provides excellent \\nmodularity for building RAG systems. \\n \\nI would recommend LangChain because of its comprehensive ecosystem, extensive \\ndocumentation, active community support, and flexibility in handling various data \\nsources and LLM integrations. \\n \\nQ20. Explain the influence of LLM context window size on RAG \\nhyperparameters. \\n \\nThe size of the LLM context window has a big impact on RAG hyperparameters, like \\nchunk size and the number of chunks that are retrieved. Larger context windows let you \\nfeed more retrieved chunks to the LLM, which increases the chance of including more \\nrelevant information. This could make the quality of the generated answers better.  \\n \\nBut after a certain point, performance gains start to go down because of problems like \\n\"lost in the middle\" and higher latency. \\n \\nQ21. How do you choose values for various LLM inference hyperparameters \\nin a RAG system? \\n \\nTemperature controls randomness—lower values give more focused and deterministic \\nresponses suitable for technical or precise tasks, while higher values make output more \\ncreative and diverse.  \\n \\nThe max tokens limit the length of the output, making sure that the answers are short or \\nlong enough depending on the use case, with a trade-off between completeness and \\nlatency. Optimal settings depend on the specific application context and are found \\nthrough iterative experimentation.  \\n \\nQ22. Compare reasoning vs. non-reasoning LLMs for RAG systems. \\n \\nReasoning LLMs such as GPT-4o1 and DeepSeek R1 are better generators in RAG \\nsystems because they have advanced \"test-time compute\" and chain-of-thought features. \\nThese unique abilities allow them to analyze the retrieved information more effectively \\nand do multi-step reasoning to come up with better answers.   \\n \\n7                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 8}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nBut non-reasoning LLMs are still cheaper and faster, which makes them a good choice \\nfor many applications. In the end, the choice between reasoning and non-reasoning \\nmodels depends on the query complexity.   \\n \\nQ23. What happens with a weak generator LLM in a RAG system? \\n \\nA weak generator LLM may find it difficult to understand the retrieved context, which \\ncould lead to answers that are incomplete or hallucinated. This makes the whole RAG \\nsystem less useful because the final answers lack coherence and factual correctness, \\neven though the context that was retrieved is good. \\n \\nSo, in a RAG setup, a strong generator LLM is necessary to convert retrieved knowledge \\ninto reliable, contextually relevant outputs. \\n \\n \\n \\n \\n                  ☕ Support the Author \\n \\nI hope you found this “RAG Interview Questions and Answers” book highly \\nuseful.   \\n \\nI’ve made this book freely available to help the AI and NLP community grow \\nand to support learners like you. If you found it helpful and would like to \\nshow your appreciation, you can buy me a coffee to keep me motivated in \\ncreating more free resources like this. \\n \\n👉 Buy Me a Coffee \\n \\nYour small gesture goes a long way in supporting my work—thank you for \\nbeing part of this journey! 🙏 \\n \\n— Kalyan KS \\n \\n \\n \\n \\n8                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 9}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ24. How do you handle ambiguous or vague user queries in RAG systems? \\n \\nIssues with ambiguous or vague user queries in RAG systems include retrieval of \\nirrelevant information, incomplete answers, and increased risk of hallucination due to \\nthe lack of specificity.  \\n \\nThe most common strategy to handle ambiguous or vague user queries is query \\nrewriting.  Query rewriting transforms unclear queries into precise and focused queries, \\nthereby enhancing retrieval quality and leading to more accurate, grounded responses. \\n \\nQ25. What are the different query transformation techniques that enhance \\nuser queries in RAG? \\n \\nDifferent query transformation techniques in RAG include query rewriting, query \\nexpansion, query decomposition, and HyDE to enhance retrieval relevance and context \\nprecision.  \\n \\nQuery Rewriting: Rewrites the initial user query to make it more specific and detailed, \\nboosting retrieval accuracy. \\n \\nQuery Expansion using Step-Back Prompting: Generates a broader, generalized version \\nof the query. \\n \\nQuery Decomposition: Divides complex queries into simpler sub-queries to ensure \\ncomprehensive coverage and more precise retrieval for each component question. \\n \\nHyDE (Hypothetical Document Embedding): Synthesizes a hypothetical answer to the \\nquery and uses it as a retrieval query to get more relevant document chunks. \\n \\nQ26. What are the pros and cons of query transformation techniques? \\n \\nQuery transformation techniques in RAG systems offer significant advantages, such as \\nimproved retrieval accuracy leading to more relevant and contextually accurate \\nresponses.  \\n \\nHowever, their downsides include increased computational cost, added latency, and \\npotential noise from overexpansion. Over expansion risks retrieving noisy or off-topic \\ndocuments, while complex methods like query decomposition require careful handling \\nto ensure subqueries align with the original intent.  \\n \\n9                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 10}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nSome strategies may also require substantial prompt engineering and continuous \\noptimization to match diverse query scenarios. Balancing effectiveness and efficiency is \\ncritical to avoid diminishing returns. \\n \\nQ27. Explain how the HyDE query transformation technique works. \\n \\nThe HyDE (Hypothetical Document Embedding) technique improves RAG retrieval by \\ntransforming the user query into a hypothetical answer before embedding it. Rather \\nthan directly searching with the query embedding, the HyDE technique utilizes a large \\nlanguage model (LLM) to create a brief, plausible document that could potentially \\nanswer the query.  \\n \\nThis synthetic document is then encoded into an embedding and used for retrieval, \\nleading to better semantic alignment with actual document chunks in the database. As a \\nresult, HyDE enhances retrieval quality, especially for vague or underspecified queries. \\n \\nQ28. Explain how the HyPE technique works in RAG. \\n \\nThe HyPE (Hypothetical Prompt Embedding) technique improves retrieval accuracy by \\naddressing the semantic mismatch between user queries and document chunks.  \\n \\nUnlike HyDE, which generates hypothetical answer documents at query time, HyPE \\nprecomputes hypothetical questions for each document chunk during the indexing \\nphase. These questions are designed to capture the key concepts in the chunk, \\ntransforming retrieval into a \"question-to-question\" matching process, which reduces \\nlatency and improves retrieval. \\n \\nQ29. Compare HyPE and HyDE techniques in RAG. \\n \\nHyDE (Hypothetical Document Embedding) and HyPE (Hypothetical Prompt \\nEmbedding) enhance RAG by addressing the semantic gap between user queries and \\ndocument chunks, but they differ in approach. \\n \\nTiming: HyPE generates hypothetical questions during indexing, while HyDE generates \\nhypothetical answer documents at query time. \\n \\nEfficiency: HyPE reduces runtime latency by avoiding LLM calls during retrieval, unlike \\nHyDE, which requires an LLM call per query. \\n \\n10                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 11}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nFocus: HyPE focuses on question-question matching, while HyDE focuses on \\nanswer-answer matching. \\n \\nWhile HyDE is flexible for diverse queries, HyPE’s pre-indexed approach is more \\nefficient for real-time applications. \\n \\nQ30. To minimize RAG system latency, which pre-retrieval enhancement \\ntechnique will you choose? \\n \\nTo minimize RAG system latency, I would choose the HyPE (Hypothetical Prompt \\nEmbedding) technique. Unlike query transformation techniques such as query \\nrewriting, query expansion, query decomposition, or HyDE, which require LLM calls at \\nquery time and increase latency, HyPE precomputes hypothetical questions for \\ndocument chunks during the indexing phase.  \\n \\nThis question-to-question matching approach reduces runtime latency by avoiding \\nreal-time LLM calls, making it more efficient for real-time applications while \\nmaintaining high retrieval accuracy. By shifting the computational effort to indexing, \\nHyPE ensures faster and more precise document retrieval. \\n \\nQ31. What are the different chunk enhancement techniques in RAG? \\n \\nThe different chunk enhancement techniques in RAG are HyPE, Contextual Chunk \\nHeader, and Document Augmentation. \\n \\nHyPE (Hypothetical Prompt Embedding) precomputes hypothetical questions for each \\ndocument chunk at indexing time, enabling retrieval by question-to-question matching, \\nwhich improves semantic alignment and retrieval accuracy without adding query-time \\nlatency. \\n \\nContextual Chunk Header adds relevant contextual information such as document titles \\nor section headings to each chunk before embedding, helping retrieval models \\nunderstand and rank chunks better when chunk text alone is ambiguous. \\n \\nDocument Augmentation enhances chunks by including additional metadata and \\nenhances retrieval quality. \\n \\n \\n11                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 12}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ32. What are the pros and cons of chunk enhancement techniques in \\nRAG? \\n \\nChunk enhancement techniques in RAG, such as HyPE, Contextual Chunk Header, and \\nDocument Augmentation, improve retrieval accuracy by enhancing semantic alignment, \\npreserving context, and bridging query-document chunk gaps, leading to better \\ngeneration performance.  \\n \\nHyPE boosts relevance through precomputed question embeddings without query-time \\nlatency, Contextual Chunk Header clarifies ambiguous chunks with document or section \\ntitles, and Document Augmentation enriches chunks with additional metadata.  \\n \\nHowever, these methods increase indexing complexity and storage requirements, \\npotentially raising computational costs. Balancing enhanced retrieval quality with \\nresource demands is a key consideration. \\n \\nQ33. Explain how the contextual chunk header technique enhances RAG \\nretrieval. \\n \\nThe Contextual Chunk Header technique in RAG enhances retrieval by adding  \\ndocument titles, section headings, or summaries to each chunk before embedding, \\nproviding critical context that clarifies ambiguous or isolated chunk content.  This \\nadditional information helps the retrieval model better understand the chunk’s \\nrelevance to a query, improving semantic alignment and ranking accuracy.  \\n \\nQ34. What are some common chunking methods used in RAG? \\n \\nCommon chunking methods used in RAG are fixed-size chunking, recursive chunking, \\nsemantic chunking, and agentic chunking.  \\n \\nFixed-size chunking divides text into uniform segments based on a predefined token or \\ncharacter length, often incorporating overlap to maintain context.  \\n \\nRecursive chunking iteratively splits text using natural separators like paragraphs or \\nsentences to preserve logical boundaries. Semantic chunking groups text based on \\nsemantic similarity using embeddings, creating coherent, meaning-based chunks.  \\n \\nAgentic chunking leverages AI agents to dynamically segment text into task-oriented, \\nsemantically coherent chunks, often with metadata to enhance retrieval relevance. \\n \\n12                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 13}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ35. What are the criteria to choose a specific chunking method in RAG? \\n \\nThe criteria for choosing a specific chunking method in RAG include the nature and \\nstructure of the source documents, capabilities of the embedding model, and the specific \\ntask or application needs.   \\n \\nFor structured or well-formatted data, semantic or agentic chunking ensures logical \\nboundaries and context preservation. The chunk size must balance between being large \\nenough to capture meaningful context and small enough to fit within model constraints \\nfor efficient processing.  \\n \\nTask specificity matters since complex tasks may require semantic or agentic chunking \\nfor better context and relevance, while simpler cases can use fixed-size chunking.  \\n \\nUltimately, the chunking method should balance retrieval relevance, context \\ncompleteness, and computational efficiency. \\n \\nQ36. Explain the pros and cons of semantic chunking. \\n \\nSemantic chunking groups text based on meaning, creating coherent chunks that \\nenhance retrieval relevance and context preservation.  \\n \\nPros: It aligns chunks with natural topic shifts, improving the quality of retrieved \\ncontent for complex queries, and reduces information loss across boundaries.  \\n \\nCons: It is computationally intensive, requiring embedding models. Additionally, it may \\nstruggle with highly complex or poorly structured documents where semantic \\nboundaries are unclear. \\n \\n \\nQ37. How does the chunking strategy differ when dealing with structured \\ndocuments (like PDFs with tables and figures) versus plain text documents? \\n \\nChunking strategies for structured documents like PDFs with tables and figures differ \\nsignificantly from plain text chunking due to the need to preserve complex layouts and \\nrelationships.  For structured documents, the chunking strategy must respect document \\nelements such as tables, figures, headers, and pages to maintain context and semantic \\nmeaning.  \\n \\n13                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 14}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nAgentic and recursive chunking are more suitable for structured documents due to their \\nflexibility in respecting structure and context. Fixed-size and semantic chunking are \\noften better suited for plain text documents where semantic coherence and simplicity \\nare prioritized. \\n \\n \\n \\n                                                        LLM Engineer Toolkit \\n 🤖 This repository contains a curated list of 120+ LLM libraries category \\nwise.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThis repository is highly useful for AI/ML Engineers.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n14                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 15}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ38. What are the possible reasons for the poor performance of a RAG \\nretriever? \\n \\nThe possible reasons for the poor performance of a RAG retriever are an outdated or \\nincomplete knowledge base, a weak retrieval model, low-quality embeddings, and lack \\nof domain-specific fine-tuning. \\n \\nAn outdated or incomplete knowledge base prevents the retriever from accessing recent \\nor relevant information, limiting answer accuracy. A weak retrieval model, such as using \\nTF-IDF or BM25 instead of dense vector models, leads to less effective retrieval of \\nrelevant context. \\n \\nLow-quality embeddings reduce the semantic understanding between queries and \\ndocument chunks, causing mismatches. Lack of domain-specific fine-tuning results in \\nretrieval errors because the embedding model doesn’t fully capture the nuances or \\nterminology of the target domain. \\n \\nQ39. What happens with a weak retriever in Retrieval-Augmented \\nGeneration (RAG) systems? \\n \\nA weak retriever in RAG systems leads to the retrieval of irrelevant or noisy document \\nchunks. This can significantly degrade the quality of generated answers, as the RAG \\ngenerator relies heavily on the retrieved context. The presence of irrelevant or noisy \\ndocument chunks in the context because of poor retrieval causes the generator model to \\nproduce answers that are inaccurate or hallucinated while still appearing fluent.  \\n \\nTherefore, strong retrievers are necessary to provide the most relevant context and \\nensure factual and relevant outputs in RAG systems. \\n \\nQ40. What are the common retrieval approaches used in RAG systems? \\n \\nCommon retrieval approaches in RAG systems include dense retrieval, sparse retrieval, \\nand hybrid retrieval. Dense retrieval uses embeddings to capture semantic similarity, \\nenabling effective query matching to relevant document chunks. Sparse retrieval relies \\non traditional methods like TF-IDF or BM25, focusing on keyword-based matching for \\nefficiency.  \\n \\nHybrid retrieval combines dense and sparse methods to balance semantic \\nunderstanding and computational speed. These approaches ensure relevant context is \\nretrieved for generating accurate responses in RAG systems. \\n15                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 16}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\n \\nQ41. What are some common challenges in RAG retrieval? \\n \\nCommon challenges in RAG retrieval include ineffective query understanding, \\nscalability issues, context fragmentation, and handling multimodal data.  Ineffective \\nquery understanding leads to misinterpreting user intent, resulting in irrelevant \\nretrieved document chunks.   \\n \\nScalability issues arise when large-scale data retrieval slows performance or overwhelms \\nthe system.   Context fragmentation happens when retrieved chunks lack sufficient \\ncontext, lowering response quality.   Handling multimodal data is challenging due to \\ncomplexities in integrating text, images, or other formats effectively. \\n \\nQ42. What are the key metrics for evaluating retrieval quality in RAG? \\n \\nKey metrics for evaluating retrieval quality in RAG systems are precision, recall, Mean \\nReciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG). \\nPrecision measures the proportion of retrieved document chunks that are relevant, \\nwhile recall assesses the proportion of relevant document chunks retrieved from the \\ntotal available.  \\n \\nMRR evaluates the ranking quality by considering the position of the first relevant \\ndocument chunks, and NDCG accounts for the relevance and ranking of retrieved \\ndocuments. These metrics collectively ensure the retriever effectively identifies and \\nranks relevant information. \\n \\nQ43. What are embeddings, and how are they utilized in RAG retrieval? \\n \\nEmbeddings are numerical vector representations of text that capture the semantic \\nmeaning \\nand \\nrelationships \\nof \\nthe \\ndata \\nin \\na \\nhigh-dimensional \\nspace. \\nIn \\nRetrieval-Augmented Generation (RAG), embeddings are used to convert both the user \\nquery and document chunks into vectors, enabling semantic search by comparing these \\nvectors for similarity.  \\n \\nThis process allows RAG systems to retrieve the most relevant and contextually \\nappropriate document chunks from a knowledge base, which are then used as context to \\ngenerate accurate and grounded responses. Thus, embeddings form the backbone of \\nRAG retrieval by enabling efficient, meaning-driven retrieval beyond simple keyword \\nmatching. \\n \\n16                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 17}, page_content=\"🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ44. What are the key considerations when choosing an embedding model \\nfor a RAG system? \\n \\nWhen choosing an embedding model for a RAG system, key considerations include  \\n \\n(i) the model's domain relevance to ensure it accurately captures domain-specific \\nsemantics,  \\n(ii) embedding dimensionality, which balances retrieval precision against computational \\nand storage costs, and  \\n(ii) embedding model performance on the specific dataset to ensure good retrieval \\nquality. This is necessary, as the real-world data often differ from academic datasets.  \\n(iv) Additionally, factors such as embedding model size, API availability, latency, cost \\nimplications, and licensing should be considered to align with infrastructure constraints \\nand use case requirements.  \\n \\nChoosing the right embedding model directly impacts the effectiveness and scalability of \\nthe RAG system. \\n \\nQ45. What is a VectorDB, and how is it utilized in RAG retrieval? \\n \\nA vector database, or VectorDB for short, is a specialized database designed to store and \\nretrieve high-dimensional vector embeddings. In RAG retrieval, VectorDB is utilized to \\nefficiently perform semantic searches by matching the vector representation of a user \\nquery with the closest vectors stored in the database, thereby retrieving the most \\ncontextually relevant document chunks.  \\n \\nVectorDBs enable scalable and fast similarity search, which is crucial for the RAG \\nsystems. \\n \\nQ46. Explain the role of ANN (Approximate Nearest Neighbor) search \\nalgorithms in RAG retrieval. \\n \\nApproximate Nearest Neighbor (ANN) search algorithms play a crucial role in RAG \\nretrieval by enabling fast and scalable search of relevant document chunks within large \\nvector databases.  Approximate Nearest Neighbor (ANN) algorithms enable fast search \\nin RAG retrieval by quickly narrowing down the search space to a small subset of \\ncandidate vectors instead of scanning all vectors.  \\n \\n17                                                          Kalyan KS (Follow on Twitter and LinkedIn)\"), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 18}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nThis reduces the number of comparisons needed, significantly speeding up retrieval \\nwhile maintaining good enough accuracy for relevant document chunks matching. This \\nbalance of speed and precision is crucial for real-time and large-scale RAG systems. \\n \\nQ47. Explain the step-by-step working of ANN algorithms for fast search in \\nRAG retrieval. \\n \\nANN algorithms for fast search in RAG retrieval involve four steps namely - Encoding, \\nIndexing, Navigating, Retrieving.  \\n \\n(i) Encoding: Convert document chunks and queries into vector representations.  \\n(ii) Indexing: Organize these vectors into a specialized data structure (like graphs or \\nhash tables) for quick lookup. \\n(iii) Navigating: Efficiently explore the index to find vectors close to the query without \\nchecking all data points.  \\n(iv) Retrieving: Return the closest approximate neighbors that provide relevant \\ninformation for RAG retrieval.  \\n \\nThis approach balances search speed and accuracy, enabling fast retrieval in large-scale \\nRAG systems.  \\n \\nQ48. What are the typical distance metrics used for similarity search in \\nvector databases, and why are they chosen? \\n \\nTypical distance metrics used in vector databases for similarity search are Euclidean \\ndistance, cosine similarity, and dot product similarity. Euclidean distance measures the \\nstraight-line distance between vectors, making it intuitive for geometric closeness. \\nCosine similarity evaluates the angle between vectors, focusing on their direction \\n(meaning) rather than magnitude.  \\n \\nDot product similarity considers both magnitude and direction. These metrics are \\nselected based on the data type and the underlying embedding model to ensure effective \\nand accurate retrieval. \\n \\nQ49. Explain why cosine similarity is preferred over other distance metrics \\nin RAG retrieval. \\n \\nCosine similarity is preferred in RAG retrieval because it measures the angle between \\nvectors, focusing on their direction (meaning) rather than magnitude. This makes it \\n18                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 19}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\neffective for textual data where the meaning lies more in the direction of the embedding \\nthan its length.  \\n \\nUnlike Euclidean distance or dot product, cosine similarity is invariant to vector length, \\nproviding stable and interpretable similarity scores. This helps RAG systems retrieve \\nrelevant document chunks even when text lengths vary, improving accuracy and \\nconsistency in semantic search. \\n \\nQ50. Compare keyword-based retrieval and semantic retrieval in RAG \\nsystems. \\n \\nKeyword-based retrieval in RAG systems relies on the exact or partial matching of \\nkeywords in a query to fetch relevant document chunks. This  offers high precision for \\nqueries with specific terms but may miss semantically related information. In contrast, \\nsemantic retrieval uses embeddings to understand the meaning behind the query and \\nretrieves conceptually relevant content even when keywords differ. \\n \\nCombining both methods can balance precision and semantic understanding for \\neffective retrieval in RAG systems. \\n \\nQ51. How does hybrid search work in the context of RAG retrieval? \\n \\nHybrid search in RAG systems combines keyword-based retrieval and semantic vector \\nsearch to leverage the strengths of both methods. It uses a weighted formula to balance \\nkeyword relevance and semantic similarity scores. This allows precise matching on exact \\nterms while also capturing conceptually related content.  \\n \\nQ52. When do you opt for hybrid search instead of semantic search? \\n \\nHybrid search is preferred over pure semantic search when there is a need to balance \\nexact keyword matches with semantic understanding, especially in scenarios where \\nusers require both precision and contextual relevance.  \\n \\nIt is ideal for domains where queries may include specific terms, codes, or entities that \\nmust be matched exactly, while also benefiting from capturing synonyms or related \\nconcepts.  \\n \\nQ53. How do you balance relevance and diversity when retrieving \\ndocument chunks for RAG? \\n19                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 20}, page_content=\"🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\n \\nThe retrieval step in RAG relies on cosine similarity to identify top-k relevant document \\nchunks. However, one downside of this approach is that it can return highly similar \\ndocument chunks, leading to redundancy.  \\n \\nBalancing relevance and diversity is crucial in RAG retrieval to include contextually \\nimportant yet diverse document chunks, preventing redundancy and capturing a \\nbroader range of perspectives. This balance helps when dealing with complex questions, \\nas different viewpoints or unique insights can improve the answer's quality while still \\nbeing accurate. \\n \\nTechniques like Maximal Marginal Relevance (MMR) help to select document chunks \\nthat are both highly relevant to the query and diverse from each other, reducing \\nredundancy. \\n \\nQ54.  How do sparse embeddings differ from dense embeddings in terms of \\nkeyword matching and retrieval interpretability? \\n \\nSparse embeddings provide interpretability and excel at exact keyword matching. These \\nembeddings represent text as high-dimensional vectors with many zeros. In this, each \\ndimension corresponds to a specific term or feature, making retrieval results more \\nunderstandable.  \\n \\nIn contrast, dense embeddings are low-dimensional, continuous vectors with mostly \\nnon-zero values learned from neural networks, capturing semantic relationships and \\ncontext beyond exact matches. This makes dense embeddings less interpretable but \\nmore effective for retrieving semantically related content where keywords do not exactly \\noverlap.  \\n \\nThus, sparse embeddings are favored for precise keyword-based retrieval and \\ninterpretability, while dense embeddings support richer, context-aware retrieval. Hybrid \\napproaches leverage the strengths of both sparse and dense embeddings to enhance \\nretrieval performance. \\n \\nQ55. How can fine-tuning embedding models improve the retriever’s \\nperformance in RAG? \\n \\n20                                                          Kalyan KS (Follow on Twitter and LinkedIn)\"), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 21}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nGeneral embedding models in RAG systems are trained on broad and diverse datasets \\nthat capture wide-ranging language patterns. However, they often lack depth in \\nvocabulary and context specific to domains. \\n \\nFine-tuning embedding models aligns the embedding space more closely with \\ndomain-specific language and context. This allows the embedding model to better \\nrepresent domain-specific terminology and jargon, which results in more precise and \\nrelevant retrieval.  \\n \\nQ56. Design a retrieval strategy for a RAG system that needs to handle \\nboth structured data (knowledge graphs) and unstructured data (text \\ndocuments) simultaneously. \\n \\nA retrieval strategy for a RAG system handling both structured (knowledge graphs) and \\nunstructured data (text documents) involves a hybrid approach combining vector-based \\nsemantic search with graph-based retrieval techniques.  \\n \\nThe system first indexes unstructured text document chunks using vector embeddings \\nfor semantic similarity search, while structured data from knowledge graphs is queried \\nusing graph traversal methods that leverage explicit entity relationships and schema \\nmetadata.  \\n \\nThe results from both sources are then fused to ensure factual precision from structured \\ndata and contextual richness from unstructured text. This combined approach enhances \\ncompleteness and reduces hallucinations in generated responses. \\n \\nQ57. Discuss the strategies to scale embeddings in RAG retrieval. \\n \\nTo scale embeddings in RAG retrieval, strategies like Matryoshka Representation \\nLearning (MRL) and quantization are highly effective. \\n \\nMRL enables flexible embeddings by training a single model to produce nested \\nrepresentations of varying sizes, allowing truncation to smaller dimensions (e.g., 64 or \\n128) with minimal performance loss, achieving up to 14x size reduction and significant \\nretrieval speed-ups.  \\n \\nQuantization reduces memory usage by compressing embeddings into lower-bit formats \\nlike float8 or int8. Combining MRL with quantization can yield up to 8x compression, \\noptimizing storage and retrieval efficiency while maintaining high accuracy for \\nlarge-scale RAG systems. \\n21                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 22}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\n \\n \\nQ58. What advantages does quantization offer over dimensionality \\nreduction for scaling embeddings? \\n \\nQuantization offers several advantages over dimensionality reduction for scaling \\nembeddings in RAG retrieval. It compresses embeddings by reducing the precision of \\nnumerical values (e.g., from float32 to int8 or float8), achieving up to 4x storage \\nreduction with minimal performance loss.  \\n \\nUnlike dimensionality reduction, which may discard important features and degrade \\naccuracy, quantization preserves the full dimensionality of embeddings, maintaining \\nricher semantic information. Additionally, quantization accelerates computation on \\nhardware optimized for lower-precision formats, improving retrieval speed.  \\n \\nThis makes it particularly effective for large-scale RAG systems where storage and \\nlatency are critical, while dimensionality reduction risks compromising retrieval quality. \\n \\nQ59. Explain the pros and cons of quantized embeddings in RAG retrieval. \\n \\nQuantized embeddings in RAG systems offer significant benefits such as drastically \\nreduced memory requirements and much faster retrieval speeds. This makes RAG \\nretrieval more efficient and scalable when dealing with large knowledge bases.  \\n \\nHowever, the trade-off is a slight drop in retrieval accuracy or relevance. Additionally, \\nquantization effectiveness can vary depending on the embedding model.  \\n \\nOverall, quantized embeddings enable cost-effective, high-speed retrieval but require \\nmanaging a controlled trade-off between resource savings and accuracy. \\n \\nQ60. Compare scalar and binary quantization for embeddings in RAG \\nretrieval. \\n \\nScalar quantization in RAG retrieval compresses embeddings by reducing the bit \\nprecision (commonly to int8), offering a moderate 4x reduction in memory usage while \\nmaintaining a good balance between retrieval accuracy and speed.  \\n \\nBinary quantization, on the other hand, converts embeddings to 1-bit vectors, achieving \\nup to 32x compression and significantly faster retrieval but at the cost of greater \\naccuracy loss.   \\n22                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 23}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\n \\nOverall, scalar quantization suits use cases prioritizing accuracy with some compression, \\nwhile binary quantization excels in large-scale, speed-critical scenarios where maximal \\nmemory efficiency outweighs some loss of precision. \\n                                   \\n \\n \\n                     🚀 AIxFunda Newsletter (free) \\nJoin 🚀 AIxFunda free newsletter to get the latest updates and interesting \\ntutorials related to Generative AI, LLMs, Agents and RAG. \\n \\n✨ Weekly GenAI updates. \\n📄 Weekly LLM, Agents and RAG paper updates. \\n📝 1 fresh blog post on an interesting topic every week. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n23                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 24}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\n \\nQ61. How does re-ranking differ from the initial retrieval process in RAG? \\n \\nThe initial retrieval process typically uses a bi-encoder that encodes queries and \\ndocuments independently and then fetches a broad set of candidates quickly.  \\n \\nThe re-ranking process reorders the retrieval results by taking the query and each \\nretrieved document chunk as a single combined input, scoring their relevance through \\ndeep interaction. This improves the final ranking quality at the cost of higher \\ncomputational overhead \\n \\nThis two-stage approach balances efficiency and accuracy by separating fast, broad \\nretrieval from slower, more exact reranking. \\n \\nQ62. Explain the pros and cons of using re-rankers in RAG. \\n \\nRe-rankers reorder search results by taking the query and each retrieved document \\nchunk as a single combined input, scoring their relevance through deep interaction \\nwithin one model pass. This helps to prioritize the most relevant information in the \\nlimited context windows in LLMs. \\n \\nHowever, re-rankers introduce increased latency and higher computational costs since \\nthey perform deep, query-chunk interaction at query time, making them less suitable for \\nreal-time or high-traffic applications. \\n \\nThe trade-off between enhanced precision and increased costs makes re-rankers ideal \\nfor specialized use cases but less suitable for applications prioritizing speed and \\ncost-efficiency. \\n \\nQ63. What are the different types of re-ranker models that can be used in \\nRAG? \\n \\nThe different types of re-ranker models used in Retrieval-Augmented Generation (RAG) \\nare \\n \\nCross-Encoder Rerankers: These models jointly encode the query and document chunk \\npair to produce a highly accurate relevance score, offering nuanced understanding of \\nrelationships but with medium computational cost. \\n \\n24                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 25}, page_content=\"🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nMulti-Vector or Late Interaction Models: Such as ColBERT, they encode queries and \\ndocument chunks separately but perform fine-grained interaction later, balancing \\nefficiency and performance with lower cost. \\n \\nLarge Language Model (LLM) Rerankers: Utilize powerful LLMs to reason about \\nquery-document chunk relevance, achieving great accuracy but incurring high \\ncomputational overhead. \\n \\nThese models vary in their performance and computational cost, and choice depends on \\nthe application's accuracy and latency requirements. \\n \\nQ64. Compare general re-rankers and instruction-following re-rankers in \\nRAG. \\n \\nGeneral re-rankers in RAG systems primarily focus on re-ranking retrieved document \\nchunks just based on their semantic relevance to the user query.  \\n \\nIn contrast, instruction-following re-rankers go a step further by dynamically adjusting \\nrankings based on additional user-provided instructions such as document recency, \\nsource reliability, or metadata criteria.  \\n \\nQ65. Why is the cross-encoder typically used as the re-ranker rather than \\nthe bi-encoder? \\n \\nThe cross-encoder is typically used as the re-ranker rather than the bi-encoder because \\nit processes the query and candidate document chunks together, allowing it to capture \\nintricate contextual interactions and provide more accurate relevance scores.  \\n \\nWhile bi-encoders encode queries and document chunks separately, enabling fast and \\nscalable retrieval of broad candidate sets, they miss detailed relationships between \\nquery-document chunk pairs.  \\n \\nCross-encoders, though slower and more resource-intensive, excel in precision, making \\nthem well-suited for re-ranking a small set of top candidates identified by the \\nbi-encoder. This combined approach balances scalability with accuracy, leveraging \\nbi-encoders for efficient candidate retrieval and cross-encoders for refined final ranking. \\n \\n \\n25                                                          Kalyan KS (Follow on Twitter and LinkedIn)\"), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 26}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ66.  A RAG system retrieves 20 candidate document chunks but can only \\nfit 5 in the LLM\\'s context window. Without re-ranking, how might this \\nlimitation affect response quality, and what specific problems would a \\nre-ranker solve? \\n \\nWhen a RAG system retrieves 20 candidate document chunks but can only fit 5 in the \\nLLM\\'s context window, the limitation can cause the model to miss critical information \\nfrom the discarded document chunks. Without re-ranking, the top 5 document chunks \\nmay not be the most relevant, leading to incomplete or less accurate answers.  \\n \\nA re-ranker solves this by analyzing and scoring all retrieved document chunks based on \\nrelevance and contextual alignment with the query, ensuring the most relevant chunks \\nare included in the limited window.  \\n \\nThis filtering reduces retrieval noise, enhances coherence, and maximizes the usefulness \\nof the input for the generative model, thereby improving the overall quality of the \\nresponse. \\n \\n \\nQ67. Describe a scenario where a BM25 retrieval might return relevant \\nchunks but in poor ranking order. How would a neural re-ranker \\nspecifically address this limitation? \\n \\nA typical scenario where BM25 retrieval yields relevant document chunks but in poor \\nranking order arises when the query uses synonyms or phrases that vary from those in \\nthe documents. This is because BM25’s exact keyword matching may surface all relevant \\nitems, but fail to prioritize those most contextually aligned due to its lack of semantic \\nunderstanding.  \\n \\nFor instance, searching for \"car maintenance\" might retrieve document chunks about \\n\"vehicle upkeep\" and \"automobile servicing,\" but BM25 may rank less relevant \\ndocument chunks higher if they have keyword overlaps rather than semantic closeness. \\nNeural re-rankers explicitly address this by leveraging deep contextual and semantic \\nsignals, reordering the candidate set to prioritize document chunks that best match the \\nquery’s intent and meaning. \\n \\n26                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 27}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ68. If your RAG system serves both simple factual queries and complex \\nanalytical questions, how would you decide when to bypass the re-ranker \\nfor efficiency while maintaining quality? \\n \\nTo decide when to bypass the re-ranker in a RAG system, queries should be classified \\nbased on complexity. Simple factual queries like \"What is the capital of France?\"  \\nrequire straightforward and well-known answers. Re-ranker can be skipped for simple \\nfactual queries, as the initial retrieval is likely to yield highly relevant results.  \\n \\nFor complex analytical questions, such as those requiring synthesis or reasoning across \\nmultiple chunks, the re-ranker should be used to ensure the most relevant chunks are \\nprioritized. \\n \\nQ69. Describe the vector pre-computation and storage strategy in a \\nbi-encoder + cross-encoder pipeline. Why can\\'t cross-encoders pre-compute \\ntext representations like bi-encoders can? \\n \\nThe RAG pipeline leverages bi-encoders for fast retrieval and cross-encoders for the \\nprecise reranking of top candidates. \\n \\nBi-encoders pre-compute chunk representations by encoding them into fixed-size dense \\nvectors offline and then storing them in a vector database for efficient retrieval.  \\n \\nCross-encoders, however, cannot pre-compute chunk representations because they \\njointly encode query-chunk pairs, capturing intricate interactions through attention \\nmechanisms, requiring both inputs at inference time to produce a relevance score.  \\n \\n \\nQ70. Compare the noise reduction capabilities of re-rankers versus simply \\nincreasing the similarity threshold in initial retrieval. When would each \\napproach be more appropriate? \\n \\nIncreasing the similarity threshold in initial retrieval reduces noise by filtering out less \\nsimilar chunks but risks missing relevant ones due to embedding limitations.  \\nRe-rankers reduce noise by prioritizing relevant chunks by deeply understanding \\nquery-chunk relevance. \\n \\nThe choice depends on the trade-off between computational cost and precision \\nrequirements. Re-rankers are preferred for high-stakes applications like legal or medical \\n27                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 28}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nsearches.  \\nIncreasing the similarity threshold is simpler and faster, suitable for \\nresource-constrained environments. \\n \\nQ71. What challenges do re-rankers face regarding computational \\noverhead and latency?  \\n \\nRe-rankers in RAG systems face significant challenges related to increased \\ncomputational overhead and latency, as each query-chunk pair must be processed.  This \\nlatency increase can hinder high-throughput environments, making re-rankers \\ncomputationally expensive compared to initial vector searches and limiting scalability. \\n \\n \\nQ72. In real-time applications with strict latency requirements, describe \\ntwo specific optimization strategies you could implement to reduce \\nre-ranking overhead while preserving most of the quality gains. \\n \\nTwo effective strategies to reduce re-ranking overhead while preserving quality gains in \\nreal-time RAG applications are  \\n \\n1) Query classifier: Deploy a query classifier to identify complex or analytical queries, \\ninvoking the re-ranker only for these while bypassing it for simple factual queries.  \\n \\n2) Model distillation: Train a smaller, faster re-ranking model to mimic the behavior of a \\nlarger, more accurate model, enabling quicker inference with minimal quality loss.  \\n \\nThese approaches balance latency and quality by minimizing computational load \\nwithout significantly compromising the relevance of retrieved results. \\n \\n \\nQ73. How would you evaluate the effectiveness of a reranker in a RAG \\nsystem? Which metrics (e.g., MRR, MAP, NDCG) would you prioritize and \\nwhy? \\n \\nThe effectiveness of a re-ranker in a RAG system is best evaluated using ranking metrics \\nthat capture how well it prioritizes relevant chunks. Mean Reciprocal Rank (MRR) is key \\nwhen the focus is on how quickly the first relevant chunk appears, ideal for \\nquestion-answering scenarios.  \\n \\n28                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 29}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nMean Average Precision (MAP) is useful when multiple relevant chunks matter, \\nmeasuring both precision and ranking quality across results. Normalized Discounted \\nCumulative Gain (NDCG) excels when relevance is graded rather than binary, rewarding \\nthe correct order of highly relevant chunks.  \\n \\n \\n                                   LLM Survey Papers Collection \\n \\nThis repo is highly useful to stay updated with LLM research.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n29                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 30}, page_content=\"🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ74. Explain the difference between Precision@k and Recall@k in the \\ncontext of RAG. When might you prefer one over the other? \\n \\nPrecision@k focuses on accuracy of the retrieval by measuring the proportion of the \\ntop-k retrieved chunks that are relevant to the query.  \\nRecall@k focuses on \\ncompleteness of the retrieval by measuring the proportion of all relevant chunks that are \\nretrieved within the top-k results. \\n \\nYou might choose Precision@k when you want to ensure high-quality, relevant chunks \\nto reduce noise. On the other hand, you might choose Recall@k when it is crucial to \\ncapture as many relevant chunks as possible. \\n \\nQ75. Why is MRR unsuitable when there are multiple relevant chunks per \\nquery, and how does MAP address this limitation? \\n \\nMRR (Mean Reciprocal Rank) considers the rank of the first relevant chunk and \\ndisregards the ranks and presence of other relevant chunks. This limitation makes MRR \\nmore appropriate for scenarios where a single chunk sufficiently answers the query.  \\n \\nIn contrast, MAP (Mean Average Precision) addresses this by averaging the precision \\nacross all relevant ranks, accounting for the presence and order of all relevant chunks. \\nHence, MAP is preferred over MRR for cases where multiple relevant chunks contribute \\nto answering a query comprehensively. \\n \\nQ76. Given a retrieval result, show how to manually calculate the MAP@5 \\n(Mean Average Precision at 5). What does MAP reveal about the retrieval \\nsystem that raw Precision does not? \\n \\nTo manually calculate MAP@5, list the top 5 retrieved items for each query and note the \\npositions where relevant items appear; then, compute precision at each relevant \\nposition (e.g., if the first relevant item appears at rank 2, precision = 1/2) and average \\nthese values to get the Average Precision (AP) for that query. Repeat this for all queries \\nand take the mean of their APs for MAP@5. \\n \\nMAP@5 reveals a retrieval system's ability to rank relevant items higher. In contrast, \\nraw Precision only measures the proportion of relevant items retrieved, ignoring their \\norder. This makes MAP@5 a better indicator of how well the system prioritizes \\nrelevance at the top of the result list. \\n \\n30                                                          Kalyan KS (Follow on Twitter and LinkedIn)\"), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 31}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\n \\nQ77. If all the relevant chunks are at the very bottom, how would \\nthis affect MRR, MAP, and NDCG metrics? Explain each. \\n \\nIf all relevant chunks are at the bottom of a ranked list for a search query, MRR (Mean \\nReciprocal Rank) would be low, as it measures the reciprocal of the rank of the first \\nrelevant chunk. MAP (Mean Average Precision) would also be low, as it averages \\nprecision across all relevant chunks, penalizing late appearances heavily due to \\nincreasing denominators in precision calculations.  \\n \\nNDCG (Normalized Discounted Cumulative Gain) would similarly be low, as it discounts \\nthe relevance scores of chunks appearing later in the ranking, reducing the cumulative \\ngain.  \\n \\nQ78. Suppose your RAG retriever gets perfect Recall@10 but low \\nPrecision@10. What problems could this cause for the downstream \\ngenerator? \\n \\nPerfect Recall@10 means all relevant chunks are retrieved within the top 10 results. Low \\nPrecision@10 indicates many of those retrieved chunks are irrelevant. If a RAG retriever \\nachieves perfect Recall@10 but low Precision@10, the downstream generator receives \\nall the relevant information mixed with much irrelevant content.  \\n \\nThis will confuse the generator model and increase the chance of generating off-topic or \\ninaccurate responses.  \\n \\nQ79. Compare and contrast “order-aware” and “order-unaware” retrieval \\nmetrics in RAG, giving examples for each from the set (Precision, Recall, \\nMRR, MAP, NDCG). \\n \\nOrder-aware retrieval metrics consider the ranking of retrieved items, emphasizing the \\nimportance of higher-ranked relevant results. For example, Mean Reciprocal Rank \\n(MRR) and Normalized Discounted Cumulative Gain (NDCG) are order-aware, as MRR \\nevaluates the rank of the first relevant item and NDCG accounts for relevance scores and \\nranking positions. \\n \\nOrder-unaware metrics focus solely on whether relevant items are retrieved and ignore \\nthe order.  Precision and Recall are order-unaware, measuring the proportion of \\n31                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 32}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nrelevant items retrieved (Precision) and the proportion of relevant items found out of all \\nrelevant items (Recall), without considering their order.  \\n \\nQ80. How would the value of NDCG@k change if all relevant chunks are \\nretrieved but in the reverse order (least to most relevant)? \\n \\nNDCG@k rewards placing highly relevant chunks at earlier ranks and applies a \\nlogarithmic discount to relevance scores at lower positions. So reversing the order \\npushes the most relevant chunks further down the list—making them less valuable in \\nthe NDCG calculation.  \\n \\nWhile all relevant items are present, their suboptimal positions reduce the overall score \\nsince NDCG is sensitive to both the presence and order of relevant items in the top k. \\nThe value of NDCG@k will decrease compared to the ideal ranking, but will remain \\nhigher than a ranking with irrelevant chunks at the top.  \\n \\nQ81. What is the significance of Context Precision@K in evaluating a RAG \\nretriever, and how does it differ from standard Precision@k in traditional \\ninformation retrieval? \\n \\nThe standard Precision@k in traditional information retrieval just measures the \\nproportion of relevant items among the top-k results and ignores the order. Unlike \\nstandard Precision@k,  Context Precision@K not only checks whether relevant chunks \\nare retrieved, but also whether they appear at higher ranks in the context.  \\n \\nContext Precision@K ensures useful information is prioritized in the retrieved context \\nwhich greatly impacts the quality of the generated answer. \\n  \\nQ82. Why does Context Precision@K use a weighted sum approach with \\nrelevance indicators, and how does this better reflect RAG retriever \\nperformance? \\n \\nContext Precision is computed as the weighted sum of Precision@k, normalized by the \\nnumber of relevant chunks.  Here the weighted sum accounts for both the presence and \\nthe rank of relevant chunks in the retrieved context. By multiplying Precision@k with \\nthe relevance indicator at each position, the metric rewards cases where relevant \\ninformation appears earlier, reflecting the importance of ranking quality.  \\n \\n32                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 33}, page_content=\"🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nThis approach better evaluates RAG retrievers, since in generative settings, not just \\nretrieving relevant chunks but placing them at higher ranks significantly impacts the \\nmodel’s ability to produce accurate answers. \\n \\nQ83. Given a retrieval result where relevant chunks appear at positions 2, \\n4, 6, and 8 out of 10 total chunks, manually calculate the Context \\nPrecision@10. What does this score tell us about the retriever's ranking \\nability? \\n \\nTo calculate Context Precision@10 with relevant chunks at positions 2, 4, 6, and 8, first \\nassign relevance indicators v_k=1 at these ranks and 0 elsewhere. Calculate \\nPrecision@k at each relevant rank: at 2, Precision@2 = 1/2 = 0.5; at 4, Precision@4 = \\n2/4 = 0.5; at 6, Precision@6 = 3/6 = 0.5; at 8, Precision@8 = 4/8 = 0.5. The weighted \\nsum is  0.5+0.5+0.5+0.5=2.  \\n \\nDividing the weighted sum by the total number of relevant chunks (4) gives Context \\nPrecision@10 = 0.5. This score indicates the retriever has moderate ranking ability, \\nretrieving relevant chunks but not consistently ranking them at the very top, thus \\nlimiting optimal prioritization of useful context. \\n \\nQ84. A RAG system achieves Context Precision@5 = 0.8. What are the \\npossible scenarios that could lead to this score? \\n \\nA Context Precision@5 score of 0.8 in a RAG system indicates that not all of the top five \\nretrieved chunks are relevant to the ground truth. This could occur if, for instance, four \\nout of five chunks are relevant (v_k = 1) and one is irrelevant (v_k = 0), leading to a \\nlower weighted sum of Precision@k when normalized by the total number of relevant \\nchunks.  \\n \\nAnother scenario might involve three relevant chunks and two irrelevant ones, with the \\nrelevant chunks ranked higher but still resulting in a score less than 1 due to the \\npresence of irrelevant chunks.  \\n \\nQ85. Explain the possible reasons for a RAG retrieval system with \\nconsistently low context precision.  \\n \\nContext Precision is computed as the weighted sum of Precision@k, normalized by the \\nnumber of relevant chunks. So low Context Precision@k scores reflect the presence of \\n33                                                          Kalyan KS (Follow on Twitter and LinkedIn)\"), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 34}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nhigh proportion of irrelevant chunks or poor ranking of relevant chunks within the top K \\nresults.  \\n \\nThis could stem from ineffective query understanding, where the system misinterprets \\nthe user’s intent, or a poorly designed retrieval algorithm that fails to prioritize chunks \\nmatching the ground truth. Additionally, a noisy or low-quality document corpus might \\ncontain few relevant chunks, causing irrelevant ones to dominate the retrieved set.  \\n \\nQ86. Compare Context Recall with traditional information retrieval recall. \\nWhy is Context Recall computed using \"ground truth claims\" rather than \\nsimply counting relevant documents? \\n \\nContext Recall in RAG retrieval differs from traditional information retrieval recall by \\nfocusing on the completeness of information through ground truth claims rather than \\nmerely counting relevant documents.  \\n \\nWhile traditional recall counts how many relevant documents are retrieved, Context \\nRecall decomposes the reference answer into individual claims and checks if these \\nspecific claims are found in the retrieved context.  \\n \\nThis approach ensures a more fine-grained evaluation of whether all necessary pieces of \\ninformation required to answer the query are present in the context or not.  \\n \\n \\nQ87. What does context precision measure in a RAG retriever, and how \\ndoes it differ from context recall? \\n \\nContext Precision in a RAG retriever measures how well the system ranks relevant \\nchunks of information higher than irrelevant ones within the retrieved context, \\nemphasizing the prioritization of useful data. In contrast, Context Recall assesses the \\ncompleteness of the retrieved context, evaluating whether all the relevant pieces of \\ninformation necessary to answer the query are present.  \\n \\nTogether, they provide complementary insights: context precision ensures useful \\ninformation is prioritized, whereas context recall ensures that no important information \\nis missed. \\n \\nQ88. In a RAG pipeline, how might context recall impact the completeness \\nof generated answers? Describe a scenario illustrating this relationship. \\n34                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 35}, page_content=\"🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\n \\nContext Recall in a RAG pipeline directly impacts the completeness of generated \\nanswers by measuring how well the retriever gathers all relevant pieces of information \\nrequired to answer the user query. \\n \\nFor instance, if a question about a historical event requires multiple claims or facts, a \\nlow Context Recall score indicates that some key facts were missed in the retrieved \\ncontext, leading to incomplete answers.  \\n \\nA high Context Recall ensures the generator (LLM) has access to all necessary \\ninformation to produce a complete and well-informed response.  \\n \\nQ89. If your retriever achieves high context precision but low context \\nrecall, what types of user queries would likely suffer most? \\n \\nIf a retriever in a RAG pipeline achieves high context precision but low context recall, \\nuser queries that require multiple distinct pieces of information or comprehensive \\ncoverage are likely to suffer most.  \\n \\nQueries, like complex multi-fact questions or those needing extensive context to answer \\nfully, will suffer because despite the retrieved chunks being relevant (high precision), \\nmany essential relevant chunks are missing overall (low recall).  \\n \\nThis results in incomplete answers, as important claims or facts are absent, limiting the \\nmodel’s ability to generate a thorough response.  \\n \\nQ90. In what situations would you prioritize Context Precision over \\nContext Recall in a RAG retriever, and how would this impact the \\ngenerator’s performance? \\n \\nIn situations where precision is critical, such as in high-risk domains like healthcare, \\nfinance, or legal applications, prioritizing Context Precision over Context Recall in a \\nRAG retriever is essential. This ensures that only the most relevant and trustworthy \\ninformation is retrieved, minimizing the risk of including irrelevant or misleading \\ncontent that could negatively impact the generator's response.  \\n \\nWhile this may limit the breadth of information (lower recall), it improves the quality \\nand reliability of the generated answers by reducing noise.  \\n \\n35                                                          Kalyan KS (Follow on Twitter and LinkedIn)\"), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 36}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ91. Describe a scenario where a RAG system might achieve high Context \\nRecall but still produce poor answers. What complementary metrics would \\nyou use alongside Context Recall to get a complete picture of retriever \\nperformance? \\n \\nA RAG system might achieve high Context Recall by retrieving most or all relevant \\ninformation pieces but still produce poor answers if the retrieved context contains noisy \\nor irrelevant data that confuses the generator.  \\n \\nTo get a complete picture of retriever performance, complementary metrics like Context \\nPrecision should be used alongside Context Recall. Context Recall along with Context \\nPrecision  ensures retrieved content is not only comprehensive but is also relevant and \\nwell-ranked. \\n \\nQ92. If your RAG retriever consistently shows Context Recall scores below \\n0.6, what are the three potential root causes? \\n \\nContext Recall scores below 0.6 mean that the retriever is missing a significant portion \\nof the relevant information required to answer user queries.  \\n \\nThe three potential root causes are  \\n(1) an incomplete or outdated knowledge base lacking necessary information,  \\n(2) ineffective embedding model or ranking algorithms causing semantically relevant \\nchunks to be missed, and  \\n(3) poor chunking strategy leading to loss of key information.  \\n \\nQ93. Why is it important for RAG systems to optimize both context \\nprecision and context recall simultaneously? What trade-offs might occur? \\n \\nIt is important for RAG systems to optimize both context precision and context recall \\nsimultaneously. This is because context precision ensures that the retrieved information \\nis highly relevant and ranked appropriately. The Context Recall metric ensures that all \\nnecessary information is included in the retrieved context so that the generator can \\noutput a complete answer.  \\n \\nThe trade-off often arises because increasing recall by retrieving more chunks may \\nintroduce irrelevant chunks, lowering precision. At the same time, focusing solely on \\nprecision might omit important information, leading to incomplete responses.  \\n \\n36                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 37}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nBalancing these metrics helps create a RAG system that retrieves relevant content \\nefficiently while covering the query comprehensively, resulting in accurate and thorough \\ngenerated answers. \\n \\nQ94. Explain why Context Relevancy is considered a \"reference-free\" metric \\nwhile Context Precision and Context Recall are \"reference-dependent.\" \\nWhen would you prefer using Context Relevancy over the other two \\nmetrics? \\n \\nContext Relevancy is considered a \"reference-free\" metric because it evaluates how \\nrelevant the retrieved context is to the user’s query without needing a reference answer. \\nIt measures the proportion of statements in the retrieved context that are relevant to the \\nquery.  \\n \\nIn contrast, Context Precision and Context Recall are \"reference-dependent\" as they \\nrequire a reference answer to determine relevance and completeness of retrieval.  \\n \\nContext Relevancy is preferred when reference answers are unavailable. The Context \\nRelevancy metric offers a way to assess retrieval quality based solely on the query and \\nretrieved context itself. This is useful for real-time scenarios where ground truth may \\nnot exist. \\n \\n \\nQ95. Describe a scenario where a RAG retriever achieves high Context \\nRelevancy but low Context Precision. What does this imply about the \\nretriever’s performance? \\n \\nA RAG retriever achieves high Context Relevancy but low Context Precision when it \\nretrieves a context where most statements are relevant to the user’s query, but the \\nrelevant chunks are ranked lower in the retrieved list, overshadowed by irrelevant ones.  \\n \\nFor example, if a query about \"machine learning algorithms\" retrieves a context with \\nmany relevant statements but places them after less relevant or noisy chunks, Context \\nRelevancy is high (most statements are query-related), but Context Precision@K is low \\ndue to poor ranking of relevant chunks.  \\n \\nThis implies the retriever is effective at fetching relevant content but struggles to \\nprioritize relevant chunks over irrelevant ones.  \\n \\n37                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 38}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\n \\nQ96. Suppose a RAG retriever retrieves all relevant chunks but includes \\nmany irrelevant ones, leading to low Context Relevancy. How would you \\nimprove the retriever to address this issue? \\n \\nA RAG retriever retrieving all relevant chunks along with many irrelevant ones results in \\nlow context relevancy scores. This can be addressed by improving the retriever by \\nrefining its filtering and ranking mechanisms. Techniques such as enhancing \\nembedding model quality, applying stricter similarity thresholds, or integrating a \\nre-ranking model can help prioritize highly relevant chunks and suppress noise.  \\n \\nAdditionally, improving the chunking strategy to create more precise and semantically \\ncoherent chunks can reduce irrelevant retrievals. These optimizations ensure retrieved \\ncontext is both comprehensive and focused on the most relevant information. \\n \\nQ97. How does the Faithfulness metric assess the quality of a RAG \\ngenerator? \\n \\nThe Faithfulness metric assesses the quality of a RAG generator by measuring how \\nfactually consistent the generated response is with the retrieved context. It is computed \\nas the ratio of claims in the response that are supported by the retrieved context to the \\ntotal number of claims.   \\n \\nA score of 1 indicates all claims are fully supported, reflecting high factual accuracy. A \\nscore of 0 shows no claims are supported, indicating complete factual inconsistency. \\nThis metric ensures the RAG system generates reliable and contextually grounded \\nresponses. \\n \\nQ98. Distinguish between Faithfulness and Context Precision metrics in RAG \\nevaluation. Why might a system have high Context Precision but low \\nFaithfulness, and what would this indicate about your pipeline? \\n \\nFaithfulness measures how factually consistent a RAG generator’s response is with the \\nretrieved context. This metric is computed as the ratio of supported claims to total \\nclaims in the response. The Context Precision metric focuses on the prioritization of \\nrelevant information by evaluating how well a retriever ranks relevant chunks within the \\ntop K.  \\n \\n38                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 39}, page_content='🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nA system might have high Context Precision but low Faithfulness if the retriever \\neffectively ranks relevant chunks highly, but the generator introduces unsupported or \\ncontradictory claims not grounded in the context. This indicates a strong retrieval stage \\nbut a flawed generation stage, where the model fails to accurately interpret or utilize the \\nretrieved information. \\n \\nQ99. A RAG system has high context precision but low faithfulness. How \\nwould you address this? \\n \\nA RAG system with high context precision and low faithfulness happens when the \\nretriever is selecting relevant chunks accurately, but the generator is producing \\nresponses with unsupported claims. To address this, one should focus on improving the \\ngenerator’s grounding and claim verification processes. \\n \\nUse stronger cross-checking mechanisms like natural language inference models or \\nfact-checking modules against the retrieved context. Additionally, tuning the generation \\nprompts to encourage reliance on the context can help increase faithfulness.  \\n \\nQ100. Why might a RAG system with perfect Context Recall still fail to \\nproduce accurate responses? How does the Faithfulness metric help \\ndiagnose this issue? \\n \\nContext recall evaluates the completeness of the retrieved context in a RAG pipeline. \\nPerfect Context Recall means the retrieved context includes all the ground truth claims.  \\nA RAG system with perfect Context Recall may still fail to produce accurate responses. \\nThis happens when the generator hallucinates and  includes claims unsupported by the \\nretrieved context.  \\n \\nThe Faithfulness metric helps diagnose this issue by measuring how many claims in the \\ngenerated response are factually supported by the retrieved context. A low or moderate \\nfaithfulness metric score indicates a less accurate response, i.e., the response includes \\nunsupported claims.   \\n \\nQ101. \\nExplain \\nhow \\nhallucinations \\nin \\nLLMs \\nspecifically \\nimpact the \\nFaithfulness metric. What techniques could you implement to improve the \\nFaithfulness metric score? \\n \\nThe faithfulness metric measures the proportion of claims in the response that are \\nbacked up by context.  \\nHallucinations reduce the faithfulness metric score by \\n39                                                          Kalyan KS (Follow on Twitter and LinkedIn)'), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 40}, page_content=\"🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nintroducing unsupported claims in the generated response. These unsupported claims \\neither contradict or have no basis in the provided context. \\n \\nTo improve Faithfulness scores, techniques such as incorporating natural language \\ninference (NLI) or fact-checking models to verify claims, using prompt engineering to \\ndiscourage unsupported generation, etc., can be used. \\n \\nQ102. How does Response Relevancy differ from Context Relevancy, and \\nwhy do you need both metrics to properly evaluate a RAG system?  \\n \\nResponse Relevancy measures how well a RAG system's generated response aligns with \\nthe user’s query by calculating the ratio of relevant statements in the response to the \\ntotal statements. Context Relevancy evaluates the relevance of retrieved context to the \\nquery by measuring the proportion of relevant statements in the context.  \\n \\nBoth metrics are essential because Context Relevancy ensures the retriever fetches \\nrelevant context, while Response Relevancy verifies that the generator produces an \\nanswer directly addressing the query.  \\n \\nA RAG system could retrieve relevant context but generate an off-topic response, or vice \\nversa. Hence, evaluating both ensures the entire RAG pipeline—retrieval and \\ngeneration—performs effectively.  \\n \\nQ103. The generator’s response mentions facts not present in the retrieved \\ncontext. Describe how faithfulness and response relevancy metrics would \\nbe impacted. \\n \\nThe faithfulness metric measures the proportion of claims in the response that are \\nbacked up by context. Therefore, the score will decrease if the LLM-generated answer \\ncontains unsupported claims. \\n \\nIn the case of the Response Relevancy metric, the score will decrease only if the \\nunsupported facts are irrelevant to the user query. Otherwise, the score will remain \\nhigh. \\n \\nThis underscores a key difference between these two metrics: Faithfulness metric looks \\nfor answer’s factual consistency with the context, while Response Relevancy assesses \\nanswer’s relevancy with the query. \\n \\n40                                                          Kalyan KS (Follow on Twitter and LinkedIn)\"), Document(metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../content/RAG_interveiw_question.pdf', 'file_path': '../content/RAG_interveiw_question.pdf', 'total_pages': 42, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-12T05:46:27+00:00', 'trapped': '', 'modDate': 'D:20251012054627Z', 'creationDate': '', 'page': 41}, page_content=\"🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ104. How does the Response Relevancy metric help evaluate whether a \\nRAG generator is addressing the user’s query effectively? \\n \\nThe Response Relevancy metric is computed as the ratio of relevant statements in the \\nresponse to the total number of statements. So, this metric checks the effectiveness of \\nthe RAG generator by measuring how well the response aligns with the user query. \\n \\nA score close to 1 means the answer directly addresses the query with little to no \\nirrelevant content. A score close to 0 means that the answer contains information that is \\nnot related to the question.   \\n \\nQ105. When evaluating RAG generator output, what are the risks of relying \\nsolely on response relevancy? How can including the faithfulness metric \\nimprove reliability? \\n \\nThe Response Relevancy metric tells you how relevant the answer is to the user query. \\nBut this metric doesn't check if the answer is based on the retrieved context, so it misses \\nfactual errors. \\n \\nThe faithfulness metric is the number of supported claims divided by the total number \\nof claims. Adding the faithfulness metric makes the system more reliable by making sure \\nthat the claims in the LLM-generated response are supported by the context. \\n   \\nThis dual evaluation ensures the RAG system delivers both relevant and factual \\nresponses, reducing the risk of misleading outputs. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n41                                                          Kalyan KS (Follow on Twitter and LinkedIn)\")]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../content\",\n",
    "    glob=\"**/*.pdf\" , #pattern to match the file\n",
    "    loader_cls= PyMuPDFLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "pdf = dir_loader.load()\n",
    "print(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef1eb3",
   "metadata": {
    "id": "a8ef1eb3"
   },
   "source": [
    "# Embedding and VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c2d032",
   "metadata": {
    "executionInfo": {
     "elapsed": 1754,
     "status": "ok",
     "timestamp": 1761370045785,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "f5c2d032"
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c21310f",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1761370045803,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "8c21310f"
   },
   "outputs": [],
   "source": [
    "from typing import List,Any\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"✅ Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "\n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da75570",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2555,
     "status": "ok",
     "timestamp": 1761370072938,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "6da75570",
    "outputId": "ba6848ca-c09d-44e0-b375-9a6f282d7de3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x7c196f96a480>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f711a4",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1761370073049,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "e8f711a4"
   },
   "outputs": [],
   "source": [
    "### vecotr store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc667431",
   "metadata": {
    "executionInfo": {
     "elapsed": 4180,
     "status": "ok",
     "timestamp": 1761370091505,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "fc667431"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "class VectorStore:\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../content/vectorstore\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "YznmtiKJidsf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1761370091614,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "YznmtiKJidsf",
    "outputId": "977e3533-2af3-4cec-ec4b-f9a8e8c7d3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x7c196f5475f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10375b4c",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1761370091621,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "10375b4c"
   },
   "outputs": [],
   "source": [
    "# convert text to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db0abfeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255,
     "referenced_widgets": [
      "1becf794a2b64c3e9ca7877643dd052b",
      "9fa34ffae4c14310b328b51d603f7f89",
      "51a949cbbca64f8693f68c3113ff37c0",
      "71753c7643644ab7a6ec5e7cec8a2d9e",
      "5b3f101cc22d402387f434cf9f4b9a39",
      "0a718cf4aee940e18de0fb5d04a8a829",
      "232df6642ca94a4fbb506b5fb965dc9a",
      "658d2155560e459c9ec2fbbb3d1cf1a3",
      "f6d19f709497433f9cd0603cb7984861",
      "c612cf5ee17a47a9b80498a7a120ba67",
      "9a3e96c7d45f469ab544d0d77cd2fa7f",
      "39bccf75a3c64e79b8714d4f017300ba",
      "421d29a438ef4c26959ca1888713090e",
      "04b27c5e69244ffab390520d47df6197",
      "fb48b38883e34365b2bf0954cee7ce6a",
      "16110d55634644d6bec5a982af631ede",
      "55a33fddec374d70b5851615532e9890",
      "ca8589213c6545819ed0713fc1b9b5eb",
      "947fd7ad2de94df0933ae5f4bccbe001",
      "7de160539cec4a2c8b9244c5eba143ec",
      "1c0c2f0da93c4fc292c880f594bab620",
      "199dee77e9d5407fb87af36bdf5c86bf"
     ]
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2722,
     "status": "ok",
     "timestamp": 1761370094346,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "db0abfeb",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "58bf7c95-bd99-421b-9baa-ae5ebde9265d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 42 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1becf794a2b64c3e9ca7877643dd052b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (42, 384)\n",
      "Adding 42 documents to vector store...\n",
      "Successfully added 42 documents to vector store\n",
      "Total documents in collection: 126\n",
      "Generating embeddings for 42 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bccf75a3c64e79b8714d4f017300ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (42, 384)\n",
      "Adding 42 documents to vector store...\n",
      "Successfully added 42 documents to vector store\n",
      "Total documents in collection: 168\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "text = [doc.page_content for doc in pdf]\n",
    "embeddings = embedding_manager.generate_embeddings(text)\n",
    "\n",
    "## stor into vector database\n",
    "vectorstore.add_documents(pdf,embeddings)\n",
    "\n",
    "## generate embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(text)\n",
    "\n",
    "## stor into vector database\n",
    "vectorstore.add_documents(pdf,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50806133",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1761370094350,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "50806133"
   },
   "outputs": [],
   "source": [
    "# Retriver Pipeline form vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcc1d92b",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1761370094355,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "fcc1d92b"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple\n",
    "class RAGRetriever:\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "\n",
    "            return retrieved_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "416858d7",
   "metadata": {
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1761370264519,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "416858d7"
   },
   "outputs": [],
   "source": [
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1wrUiwt4oDR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "fadfed920d6c479493c1ab1a07935b88",
      "df85d15c27d74ac5afd5f98afeff1a63",
      "0df18fec09e14d358bea2efb20031d0a",
      "fc06f622e50a4223b587ebaa2b9147a7",
      "d6215b5bd5204aa595a736cd89c7720d",
      "f4aed730bd0d4560b8d5531c4bbd41c9",
      "e6e6205d257c4da292a6a1c41d5c6d6d",
      "ca3be090299048178f0b349cf38ba257",
      "90c8d432af6c4c569fae0351143db7cb",
      "5379c43c344d43c5b388c2fb425613e9",
      "f8b0904c3e264a708c65c74465972641"
     ]
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1761370094386,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "e1wrUiwt4oDR",
    "outputId": "14eebd20-d971-45a5-d14e-10e0182d339f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'requirement of RAG when LLMs are already powerful'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadfed920d6c479493c1ab1a07935b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_f544ef77_1',\n",
       "  'content': '🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ1. Explain the requirement of RAG when LLMs are already powerful. \\n \\nLLMs are powerful, as they are trained on large volumes of data using sophisticated \\ntechniques. However, LLMs because of knowledge cutoff (static knowledge), struggle to \\nanswer queries related to the latest events or the data not present in their training \\ncorpus.  \\n \\nRAG addresses this challenge by retrieving relevant context from external knowledge \\nsources, which allows LLMs to provide accurate responses. This is why RAG is essential \\nfor LLM-based applications that need to be accurate. Otherwise, LLMs alone might \\nprovide you answers that are incomplete or outdated. \\n \\nQ2. Is RAG still relevant in the era of long context LLMs? \\n \\nRAG is still important even with long context LLMs. This is because long-context LLMs \\nwithout RAG have three big problems: \"lost in the middle,\", high API costs, and \\nincreased latency. \\n \\nLong-context LLMs often struggle to find the most relevant  information in large \\ncontexts, which hurts the quality of generated responses. Furthermore, processing \\nlengthy sequences in each API call results in high latency and high API costs. \\n \\nRAG addresses these issues by providing the most relevant information from external \\nknowledge sources. So, you still need RAG to get accurate and cost-efficient responses, \\neven with long context LLMs. \\n \\nQ3. What are the fundamental challenges of RAG systems? \\n \\nRAG is powerful, but it has to deal with the following challenges: \\n \\nScalability: Searching and retrieving from large, dynamic knowledge sources quickly \\nand efficiently requires a lot of computing power and well-optimized indexing, which \\ncan be expensive or take a long time. \\n \\nLatency - The two-step process (retrieval then generation) can cause delays, making it \\nless suitable for real-time applications without careful optimization. \\n \\nHallucination Risk - Even with retrieval, the model might generate plausible but \\nunsupported details if the retrieved data is ambiguous or insufficient. \\n \\n1                                                          Kalyan KS (Follow on Twitter and LinkedIn)',\n",
       "  'metadata': {'source': '../content/RAG_interveiw_question.pdf',\n",
       "   'keywords': '',\n",
       "   'format': 'PDF 1.7',\n",
       "   'trapped': '',\n",
       "   'total_pages': 42,\n",
       "   'subject': '',\n",
       "   'page': 1,\n",
       "   'moddate': '2025-10-12T05:46:27+00:00',\n",
       "   'producer': 'iLovePDF',\n",
       "   'creator': '',\n",
       "   'creationdate': '',\n",
       "   'title': '',\n",
       "   'modDate': 'D:20251012054627Z',\n",
       "   'content_length': 2245,\n",
       "   'file_path': '../content/RAG_interveiw_question.pdf',\n",
       "   'creationDate': '',\n",
       "   'doc_index': 1,\n",
       "   'author': ''},\n",
       "  'similarity_score': 0.16129523515701294,\n",
       "  'distance': 0.8387047648429871,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_cc8f1220_1',\n",
       "  'content': '🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ1. Explain the requirement of RAG when LLMs are already powerful. \\n \\nLLMs are powerful, as they are trained on large volumes of data using sophisticated \\ntechniques. However, LLMs because of knowledge cutoff (static knowledge), struggle to \\nanswer queries related to the latest events or the data not present in their training \\ncorpus.  \\n \\nRAG addresses this challenge by retrieving relevant context from external knowledge \\nsources, which allows LLMs to provide accurate responses. This is why RAG is essential \\nfor LLM-based applications that need to be accurate. Otherwise, LLMs alone might \\nprovide you answers that are incomplete or outdated. \\n \\nQ2. Is RAG still relevant in the era of long context LLMs? \\n \\nRAG is still important even with long context LLMs. This is because long-context LLMs \\nwithout RAG have three big problems: \"lost in the middle,\", high API costs, and \\nincreased latency. \\n \\nLong-context LLMs often struggle to find the most relevant  information in large \\ncontexts, which hurts the quality of generated responses. Furthermore, processing \\nlengthy sequences in each API call results in high latency and high API costs. \\n \\nRAG addresses these issues by providing the most relevant information from external \\nknowledge sources. So, you still need RAG to get accurate and cost-efficient responses, \\neven with long context LLMs. \\n \\nQ3. What are the fundamental challenges of RAG systems? \\n \\nRAG is powerful, but it has to deal with the following challenges: \\n \\nScalability: Searching and retrieving from large, dynamic knowledge sources quickly \\nand efficiently requires a lot of computing power and well-optimized indexing, which \\ncan be expensive or take a long time. \\n \\nLatency - The two-step process (retrieval then generation) can cause delays, making it \\nless suitable for real-time applications without careful optimization. \\n \\nHallucination Risk - Even with retrieval, the model might generate plausible but \\nunsupported details if the retrieved data is ambiguous or insufficient. \\n \\n1                                                          Kalyan KS (Follow on Twitter and LinkedIn)',\n",
       "  'metadata': {'creationDate': '',\n",
       "   'page': 1,\n",
       "   'total_pages': 42,\n",
       "   'file_path': '../content/RAG_interveiw_question.pdf',\n",
       "   'creationdate': '',\n",
       "   'format': 'PDF 1.7',\n",
       "   'moddate': '2025-10-12T05:46:27+00:00',\n",
       "   'subject': '',\n",
       "   'creator': '',\n",
       "   'modDate': 'D:20251012054627Z',\n",
       "   'keywords': '',\n",
       "   'author': '',\n",
       "   'producer': 'iLovePDF',\n",
       "   'content_length': 2245,\n",
       "   'doc_index': 1,\n",
       "   'trapped': '',\n",
       "   'title': '',\n",
       "   'source': '../content/RAG_interveiw_question.pdf'},\n",
       "  'similarity_score': 0.16129523515701294,\n",
       "  'distance': 0.8387047648429871,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_488b7eb9_1',\n",
       "  'content': '🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ1. Explain the requirement of RAG when LLMs are already powerful. \\n \\nLLMs are powerful, as they are trained on large volumes of data using sophisticated \\ntechniques. However, LLMs because of knowledge cutoff (static knowledge), struggle to \\nanswer queries related to the latest events or the data not present in their training \\ncorpus.  \\n \\nRAG addresses this challenge by retrieving relevant context from external knowledge \\nsources, which allows LLMs to provide accurate responses. This is why RAG is essential \\nfor LLM-based applications that need to be accurate. Otherwise, LLMs alone might \\nprovide you answers that are incomplete or outdated. \\n \\nQ2. Is RAG still relevant in the era of long context LLMs? \\n \\nRAG is still important even with long context LLMs. This is because long-context LLMs \\nwithout RAG have three big problems: \"lost in the middle,\", high API costs, and \\nincreased latency. \\n \\nLong-context LLMs often struggle to find the most relevant  information in large \\ncontexts, which hurts the quality of generated responses. Furthermore, processing \\nlengthy sequences in each API call results in high latency and high API costs. \\n \\nRAG addresses these issues by providing the most relevant information from external \\nknowledge sources. So, you still need RAG to get accurate and cost-efficient responses, \\neven with long context LLMs. \\n \\nQ3. What are the fundamental challenges of RAG systems? \\n \\nRAG is powerful, but it has to deal with the following challenges: \\n \\nScalability: Searching and retrieving from large, dynamic knowledge sources quickly \\nand efficiently requires a lot of computing power and well-optimized indexing, which \\ncan be expensive or take a long time. \\n \\nLatency - The two-step process (retrieval then generation) can cause delays, making it \\nless suitable for real-time applications without careful optimization. \\n \\nHallucination Risk - Even with retrieval, the model might generate plausible but \\nunsupported details if the retrieved data is ambiguous or insufficient. \\n \\n1                                                          Kalyan KS (Follow on Twitter and LinkedIn)',\n",
       "  'metadata': {'content_length': 2245,\n",
       "   'subject': '',\n",
       "   'trapped': '',\n",
       "   'doc_index': 1,\n",
       "   'author': '',\n",
       "   'creationdate': '',\n",
       "   'total_pages': 42,\n",
       "   'file_path': '../content/RAG_interveiw_question.pdf',\n",
       "   'title': '',\n",
       "   'keywords': '',\n",
       "   'producer': 'iLovePDF',\n",
       "   'creationDate': '',\n",
       "   'moddate': '2025-10-12T05:46:27+00:00',\n",
       "   'source': '../content/RAG_interveiw_question.pdf',\n",
       "   'modDate': 'D:20251012054627Z',\n",
       "   'creator': '',\n",
       "   'format': 'PDF 1.7',\n",
       "   'page': 1},\n",
       "  'similarity_score': 0.16129523515701294,\n",
       "  'distance': 0.8387047648429871,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_12b5b2fd_1',\n",
       "  'content': '🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ1. Explain the requirement of RAG when LLMs are already powerful. \\n \\nLLMs are powerful, as they are trained on large volumes of data using sophisticated \\ntechniques. However, LLMs because of knowledge cutoff (static knowledge), struggle to \\nanswer queries related to the latest events or the data not present in their training \\ncorpus.  \\n \\nRAG addresses this challenge by retrieving relevant context from external knowledge \\nsources, which allows LLMs to provide accurate responses. This is why RAG is essential \\nfor LLM-based applications that need to be accurate. Otherwise, LLMs alone might \\nprovide you answers that are incomplete or outdated. \\n \\nQ2. Is RAG still relevant in the era of long context LLMs? \\n \\nRAG is still important even with long context LLMs. This is because long-context LLMs \\nwithout RAG have three big problems: \"lost in the middle,\", high API costs, and \\nincreased latency. \\n \\nLong-context LLMs often struggle to find the most relevant  information in large \\ncontexts, which hurts the quality of generated responses. Furthermore, processing \\nlengthy sequences in each API call results in high latency and high API costs. \\n \\nRAG addresses these issues by providing the most relevant information from external \\nknowledge sources. So, you still need RAG to get accurate and cost-efficient responses, \\neven with long context LLMs. \\n \\nQ3. What are the fundamental challenges of RAG systems? \\n \\nRAG is powerful, but it has to deal with the following challenges: \\n \\nScalability: Searching and retrieving from large, dynamic knowledge sources quickly \\nand efficiently requires a lot of computing power and well-optimized indexing, which \\ncan be expensive or take a long time. \\n \\nLatency - The two-step process (retrieval then generation) can cause delays, making it \\nless suitable for real-time applications without careful optimization. \\n \\nHallucination Risk - Even with retrieval, the model might generate plausible but \\nunsupported details if the retrieved data is ambiguous or insufficient. \\n \\n1                                                          Kalyan KS (Follow on Twitter and LinkedIn)',\n",
       "  'metadata': {'creator': '',\n",
       "   'file_path': '../content/RAG_interveiw_question.pdf',\n",
       "   'page': 1,\n",
       "   'source': '../content/RAG_interveiw_question.pdf',\n",
       "   'trapped': '',\n",
       "   'content_length': 2245,\n",
       "   'modDate': 'D:20251012054627Z',\n",
       "   'author': '',\n",
       "   'title': '',\n",
       "   'subject': '',\n",
       "   'total_pages': 42,\n",
       "   'producer': 'iLovePDF',\n",
       "   'creationdate': '',\n",
       "   'moddate': '2025-10-12T05:46:27+00:00',\n",
       "   'format': 'PDF 1.7',\n",
       "   'keywords': '',\n",
       "   'creationDate': '',\n",
       "   'doc_index': 1},\n",
       "  'similarity_score': 0.16129523515701294,\n",
       "  'distance': 0.8387047648429871,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_e89f6f14_6',\n",
       "  'content': '🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nAdditionally, inference speed, infrastructure compatibility, and licensing terms also play \\na key role in deployment decisions for real-world RAG solutions. \\n \\nQ17. How is the prompt provided to the LLM in a RAG system different from \\na standard, non-RAG prompt? \\n \\nThe prompt provided to the LLM without a RAG setup includes only the user query and \\nthe optional instructions. Here, the LLM generates the response based on its knowledge \\ngained during training. \\n \\nThe prompt provided to the LLM with the RAG setup includes the user query, \\ninstructions, and relevant context. Here, the LLM generates the response as per the \\ninstructions solely based on the provided relevant context.  \\n \\nQ18. What are the key hyperparameters in a RAG pipeline? \\n \\nChunk size, chunk overlap, embedding dimensionality, retrieval top-k, and retrieval \\nthreshold are some of the most important hyperparameters for retrieval in RAG. \\nTemperature and max output length are two important hyperparameters for RAG \\ngeneration. \\n \\nThe chunk size determines how much text is put into a segment before embedding, \\ninfluencing the context granularity retrieved. Chunk overlap repeats a set of tokens at \\nchunk boundaries, helping preserve important context across segments. Embedding \\ndimensionality is the vector size used to represent text, which affects retrieval precision \\nand database efficiency. \\n \\nRetrieval top-k sets the number of most similar chunks returned, directly impacting \\nrecall and context diversity in the response. The retrieval threshold is a similarity cutoff \\nthat filters retrieved results, ensuring only relevant chunks are selected. \\n \\nTemperature controls the randomness of generated text, balancing creativity and \\ndeterminism in model outputs. Max output length limits the number of tokens \\ngenerated, managing the verbosity and computational cost of responses. \\n \\nQ19. What are the popular frameworks to implement a RAG system? Justify \\nyour choice of framework. \\n \\nLangChain, LlamaIndex, and Haystack are the most popular frameworks for RAG \\nimplementation. LangChain is great for custom pipelines, and LlamaIndex is great for \\n6                                                          Kalyan KS (Follow on Twitter and LinkedIn)',\n",
       "  'metadata': {'creationdate': '',\n",
       "   'moddate': '2025-10-12T05:46:27+00:00',\n",
       "   'producer': 'iLovePDF',\n",
       "   'doc_index': 6,\n",
       "   'content_length': 2363,\n",
       "   'file_path': '../content/RAG_interveiw_question.pdf',\n",
       "   'trapped': '',\n",
       "   'modDate': 'D:20251012054627Z',\n",
       "   'source': '../content/RAG_interveiw_question.pdf',\n",
       "   'creationDate': '',\n",
       "   'keywords': '',\n",
       "   'creator': '',\n",
       "   'page': 6,\n",
       "   'total_pages': 42,\n",
       "   'author': '',\n",
       "   'format': 'PDF 1.7',\n",
       "   'title': '',\n",
       "   'subject': ''},\n",
       "  'similarity_score': 0.15168213844299316,\n",
       "  'distance': 0.8483178615570068,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve('requirement of RAG when LLMs are already powerful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "v4DZJlNGhA-t",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1761370094395,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "v4DZJlNGhA-t"
   },
   "outputs": [],
   "source": [
    "# Integeration the context with groqapi through the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "xq_I7OGj43lr",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1761370094401,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "xq_I7OGj43lr"
   },
   "outputs": [],
   "source": [
    "GROQ_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "kWH7ofeYkdAk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1761370150715,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "kWH7ofeYkdAk",
    "outputId": "1eb5587e-7579-4fbf-94d7-9fa64bbacbba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "jhBJpu5lknc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1761370150813,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "jhBJpu5lknc8",
    "outputId": "3438c4c5-25b9-4609-8efc-33c3570911e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "GROQ_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lY8Yd45s5ebV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1761370150960,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "lY8Yd45s5ebV",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b85a5e95-66bd-4925-f615-2476f907defb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
      "Requirement already satisfied: groq<1.0.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.33.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (1.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (2.11.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.15.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-groq) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-groq) (0.4.37)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-groq) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-groq) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-groq) (8.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq<1.0.0,>=0.30.0->langchain-groq) (3.11)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-groq) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-groq) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-groq) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-groq) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-groq) (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "uQJ3vhTJjlbu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1761370151206,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "uQJ3vhTJjlbu",
    "outputId": "33a74d36-d8ba-41e9-9c61-ff98c1952cd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "iuqljh3PjsFO",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1761370151211,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "iuqljh3PjsFO"
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "\n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "LzHXc76mk0Yd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "d2fba97f9a64455b947ca858a7e2184c",
      "c78f680bdc9b4bfb810cd96df3b991ac",
      "e643ede8d01d4de99526c3780b259452",
      "417549e4b2ea4abebe692599873f4f53",
      "75df689f9fde420794aa361f06b951cc",
      "747207e0d44f4e3fb2b39754e60e2931",
      "7f49d336e4064f89817613c6d9b839fc",
      "ba49aa7b95e546b99dc40f1a44ce564a",
      "05a4e81e56a44e0aa6b2b0863183d54a",
      "1a277ddbf1994604b80bfdce965576be",
      "347dd82a33514a3a8400050f460d27fd"
     ]
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1761370151245,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "LzHXc76mk0Yd",
    "outputId": "09e95bf9-5956-4f6f-f53d-cd74d32388d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is the requirement of RAG when LLMs are already powerful?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fba97f9a64455b947ca858a7e2184c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "The requirement of RAG when LLMs are already powerful is to address the challenge of knowledge cutoff (static knowledge). LLMs struggle to answer queries related to the latest events or data not present in their training corpus. RAG retrieves relevant context from external knowledge sources, allowing LLMs to provide accurate responses.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is the requirement of RAG when LLMs are already powerful?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "-v12Cbi0lA6F",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1761370151263,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "-v12Cbi0lA6F"
   },
   "outputs": [],
   "source": [
    "# Enchance Rag pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5RLZZ_3q1K9s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520,
     "referenced_widgets": [
      "9d5de736341f41ddb3c3936fa6feec33",
      "cb3e160e0fb141acb89cbb112541a4bd",
      "a806c587c60f43acb7ff23ef844e68bd",
      "5ba69c91d0694cabb5cbdda66fe52a1f",
      "7645cde6b10845a1a6afb97b9be99b26",
      "b02233080b0a4850b1acd7f4146cc985",
      "1d47cfb519ce45e89999975095df441b",
      "6adcc5a7b82943d098cf8457605005b7",
      "f4bddee67a0d4a85a0f05680008b2a1f",
      "432b3fa010174c1fb0872f30611cbd27",
      "81d58f99b601463aa29439a986c6cd29"
     ]
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1761370330071,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "5RLZZ_3q1K9s",
    "outputId": "b4b8e202-83b8-4870-c204-51f84cb3b04f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'need for rag when LLM exists'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5de736341f41ddb3c3936fa6feec33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Answer: RAG (Retrieval-Augmented Generation) is still essential even when LLMs (Large Language Models) are powerful because LLMs have a knowledge cutoff (static knowledge) and struggle to answer queries related to the latest events or data not present in their training corpus. RAG addresses this challenge by retrieving relevant context from external knowledge sources, allowing LLMs to provide accurate responses.\n",
      "Sources: [{'source': '../content/RAG_interveiw_question.pdf', 'page': 1, 'score': 0.2769176959991455, 'preview': '🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ1. Explain the requirement of RAG when LLMs are already powerful. \\n \\nLLMs are powerful, as they are trained on large volumes of data using sophisticated \\ntechniques. Howeve...'}, {'source': '../content/RAG_interveiw_question.pdf', 'page': 1, 'score': 0.2769176959991455, 'preview': '🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ1. Explain the requirement of RAG when LLMs are already powerful. \\n \\nLLMs are powerful, as they are trained on large volumes of data using sophisticated \\ntechniques. Howeve...'}, {'source': '../content/RAG_interveiw_question.pdf', 'page': 1, 'score': 0.2769176959991455, 'preview': '🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \\n \\nQ1. Explain the requirement of RAG when LLMs are already powerful. \\n \\nLLMs are powerful, as they are trained on large volumes of data using sophisticated \\ntechniques. Howeve...'}]\n",
      "Confidence: 0.2769176959991455\n",
      "Context Preview: 🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n",
      " \n",
      "Q1. Explain the requirement of RAG when LLMs are already powerful. \n",
      " \n",
      "LLMs are powerful, as they are trained on large volumes of data using sophisticated \n",
      "techniques. However, LLMs because of knowledge cutoff (static knowledge), struggle to \n",
      "answer queries related to the latest events or the data not present in their training \n",
      "corpus.  \n",
      " \n",
      "RAG addresses this challenge by retrieving relevant context from external knowledge \n",
      "sources, which allows LLMs to provide accurate responses. This is why RAG is essential \n",
      "for LLM-based applications that need to be accurate. Otherwise, LLMs alone might \n",
      "provide you answers that are incomplete or outdated. \n",
      " \n",
      "Q2. Is RAG still relevant in the era of long context LLMs? \n",
      " \n",
      "RAG is still important even with long context LLMs. This is because long-context LLMs \n",
      "without RAG have three big problems: \"lost in the middle,\", high API cos\n"
     ]
    }
   ],
   "source": [
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "\n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "\n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "\n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"need for rag when LLM exists\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "mQxWpA5n1SgW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c14dec6946e644628dbc750341a911dd",
      "e1056f0b359f4ab5942dd27b132bf9c4",
      "2cedf63aa58e497285c332251347fdf1",
      "28e199bf83274223b602a120e1f7a61b",
      "6788360aca254fcbae3fc5eb0004cdc6",
      "4651f67e2f504aae93fb76d5dfafb488",
      "46412b2389434c08945cddeebcaaeda6",
      "3daa160a7b654e61b0fb9a771309c36d",
      "ec42626d450c4f5680b282f258ea2c0e",
      "44f9a69112754fbb8435efda6e2bb80c",
      "59744726d85e4becad6fa705acc15806"
     ]
    },
    "executionInfo": {
     "elapsed": 5015,
     "status": "ok",
     "timestamp": 1761370440990,
     "user": {
      "displayName": "Syed Muhammad Meesam Abbas",
      "userId": "09750389743023066861"
     },
     "user_tz": -300
    },
    "id": "mQxWpA5n1SgW",
    "outputId": "a789b1fe-8d9f-4c0d-c05c-e9b852998488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is rag and llm and why they needed'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14dec6946e644628dbc750341a911dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n",
      " \n",
      "Q1. Explain the requirement of RAG when LLMs are already powerful. \n",
      " \n",
      "LLMs are powerful, as they are trained on large volumes of data using sophisticated \n",
      "techniques. However, LLMs because of knowledge cutoff (static knowledge), struggle to \n",
      "answer queries related to the latest events or the data not present in their training \n",
      "corpus.  \n",
      " \n",
      "RAG addresses this challenge by retrieving relevant context from external knowledge \n",
      "sources, which allows LLMs to provide accurate responses. This is why RAG is essential \n",
      "for LLM-based applications that need to be accurate. Otherwise, LLMs alone might \n",
      "provide you answers that are incomplete or outdated. \n",
      " \n",
      "Q2. Is RAG still relevant in the era of long context LLMs? \n",
      " \n",
      "RAG is still important even with long context LLMs. This is because long-context LLMs \n",
      "without RAG have three big problems: \"lost in the middle,\", high API costs, and \n",
      "increased latency. \n",
      " \n",
      "Long-context LLMs often struggle to find the most relevant  information in large \n",
      "contexts, which hurts the quality of generated responses. Furthermore, processing \n",
      "lengthy sequences in each API call results in high latency and high API costs. \n",
      " \n",
      "RAG addresses these issues by providing the most relevant information from external \n",
      "knowledge sources. So, you still need RAG to get accurate and cost-efficient responses, \n",
      "even with long context LLMs. \n",
      " \n",
      "Q3. What are the fundamental challenges of RAG systems? \n",
      " \n",
      "RAG is powerful, but it has to deal with the following challenges: \n",
      " \n",
      "Scalability: Searching and retrieving from large, dynamic knowledge sources quickly \n",
      "and efficiently requires a lot of computing power and well-optimized indexing, which \n",
      "can be expensive or take a long time. \n",
      " \n",
      "Latency - The two-step process (retrieval then generation) can cause delays, making it \n",
      "less suitable for real-time applications without careful optimization. \n",
      " \n",
      "Hallucination Risk - Even with retrieval, the model might generate plausible but \n",
      "unsupported details if the retrieved data is ambiguous or insufficient. \n",
      " \n",
      "1                                                          Kalyan KS (Follow on Twitter and LinkedIn)\n",
      "\n",
      "🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n",
      " \n",
      "Q1. Explain the requirement of RAG when LLMs are already powerful. \n",
      " \n",
      "LLMs are powerful, as they are trained on large volumes of data using sophisticated \n",
      "techniques. However, LLMs because of knowledge cutoff (static knowledge), struggle to \n",
      "answer queries related to the latest events or the data not present in their training \n",
      "corpus.  \n",
      " \n",
      "RAG addresses this challenge by retrieving relevant context from external knowledge \n",
      "sources, which allows LLMs to provide accurate responses. This is why RAG is essential \n",
      "for LLM-based applications that need to be accurate. Otherwise, LLMs alone might \n",
      "provide you answers that are incomplete or outdated. \n",
      " \n",
      "Q2. Is RAG still relevant in the era of long context LLMs? \n",
      " \n",
      "RAG is still important even with long context LLMs. This is because long-context LLMs \n",
      "without RAG have three big problems: \"lost in the middle,\", high API costs, and \n",
      "increased latency. \n",
      " \n",
      "Long-context LLMs often struggle to find the most relevant  information in large \n",
      "contexts, which hurts the quality of generated responses. Furthermore, processing \n",
      "lengthy sequences in each API call results in high latency and high API costs. \n",
      " \n",
      "RAG addresses these issues by providing the most relevant information from external \n",
      "knowledge sources. So, you still need RAG to get accurate and cost-efficient responses, \n",
      "even with long context LLMs. \n",
      " \n",
      "Q3. What are the fundamental challenges of RAG systems? \n",
      " \n",
      "RAG is powerful, but it has to deal with the following challenges: \n",
      " \n",
      "Scalability: Searching and retrieving from large, dynamic knowledge sources quickly \n",
      "and efficiently requires a lot of computing power and well-optimized indexing, which \n",
      "can be expensive or take a long time. \n",
      " \n",
      "Latency - The two-step process (retrieval then generation) can cause delays, making it \n",
      "less suitable for real-time applications without careful optimization. \n",
      " \n",
      "Hallucination Risk - Even with retrieval, the model might generate plausible but \n",
      "unsupported details if the retrieved data is ambiguous or insufficient. \n",
      " \n",
      "1                                                          Kalyan KS (Follow on Twitter and LinkedIn)\n",
      "\n",
      "🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n",
      " \n",
      "Q1. Explain the requirement of RAG when LLMs are already powerful. \n",
      " \n",
      "LLMs are powerful, as they are trained on large volumes of data using sophisticated \n",
      "techniques. However, LLMs because of knowledge cutoff (static knowledge), struggle to \n",
      "answer queries related to the latest events or the data not present in their training \n",
      "corpus.  \n",
      " \n",
      "RAG addresses this challenge by retrieving relevant context from external knowledge \n",
      "sources, which allows LLMs to provide accurate responses. This is why RAG is essential \n",
      "for LLM-based applications that need to be accurate. Otherwise, LLMs alone might \n",
      "provide you answers that are incomplete or outdated. \n",
      " \n",
      "Q2. Is RAG still relevant in the era of long context LLMs? \n",
      " \n",
      "RAG is still important even with long context LLMs. This is because long-context LLMs \n",
      "without RAG have three big problems: \"lost in the middle,\", high API costs, and \n",
      "increased latency. \n",
      " \n",
      "Long-context LLMs often struggle to find the most relevant  information in large \n",
      "contexts, which hurts the quality of generated responses. Furthermore, processing \n",
      "lengthy sequences in each API call results in high latency and high API costs. \n",
      " \n",
      "RAG addresses these issues by providing the most relevant information from external \n",
      "knowledge sources. So, you still need RAG to get accurate and cost-efficient responses, \n",
      "even with long context LLMs. \n",
      " \n",
      "Q3. What are the fundamental challenges of RAG systems? \n",
      " \n",
      "RAG is powerful, but it has to deal with the following challenges: \n",
      " \n",
      "Scalability: Searching and retrieving from large, dynamic knowledge sources quickly \n",
      "and efficiently requires a lot of computing power and well-optimized indexing, which \n",
      "can be expensive or take a long time. \n",
      " \n",
      "Latency - The two-step process (retrieval then generation) can cause delays, making it \n",
      "less suitable for real-time applications without careful optimization. \n",
      " \n",
      "Hallucination Risk - Even with retrieval, the model might generate plausible but \n",
      "unsupported details if the retrieved data is ambiguous or insufficient. \n",
      " \n",
      "1                                                          Kalyan KS (Follow on Twitter and LinkedIn)\n",
      "\n",
      "Question: what is rag and llm and why they needed\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: **What is RAG and LLM?**\n",
      "\n",
      "- **RAG (Retrieval-Augmented Generation)**: A technique that combines the strengths of Retrieval and Generation models. It retrieves relevant context from external knowledge sources and uses it to generate accurate responses.\n",
      "- **LLM (Large Language Model)**: A type of artificial intelligence model that is trained on large volumes of data using sophisticated techniques. It is powerful but struggles with knowledge cutoff (static knowledge) and answering queries related to the latest events or data not present in its training corpus.\n",
      "\n",
      "**Why are they needed?**\n",
      "\n",
      "RAG and LLM are needed because LLMs, despite their power, have limitations. They struggle to answer queries related to the latest events or data not present in their training corpus due to knowledge cutoff. RAG addresses this challenge by retrieving relevant context from external knowledge sources, allowing LLMs to provide accurate responses.\n",
      "\n",
      "Citations:\n",
      "[1] ../content/RAG_interveiw_question.pdf (page 1)\n",
      "[2] ../content/RAG_interveiw_question.pdf (page 1)\n",
      "[3] ../content/RAG_interveiw_question.pdf (page 1)\n",
      "Summary: Here's a 2-sentence summary:\n",
      "\n",
      "RAG (Retrieval-Augmented Generation) and LLM (Large Language Model) are techniques and models used to improve the accuracy of language models. RAG addresses the limitations of LLMs by retrieving relevant context from external knowledge sources, enabling them to provide accurate responses to queries related to the latest events or data not present in their training corpus.\n",
      "History: {'question': 'what is rag and llm and why they needed', 'answer': '**What is RAG and LLM?**\\n\\n- **RAG (Retrieval-Augmented Generation)**: A technique that combines the strengths of Retrieval and Generation models. It retrieves relevant context from external knowledge sources and uses it to generate accurate responses.\\n- **LLM (Large Language Model)**: A type of artificial intelligence model that is trained on large volumes of data using sophisticated techniques. It is powerful but struggles with knowledge cutoff (static knowledge) and answering queries related to the latest events or data not present in its training corpus.\\n\\n**Why are they needed?**\\n\\nRAG and LLM are needed because LLMs, despite their power, have limitations. They struggle to answer queries related to the latest events or data not present in their training corpus due to knowledge cutoff. RAG addresses this challenge by retrieving relevant context from external knowledge sources, allowing LLMs to provide accurate responses.', 'sources': [{'source': '../content/RAG_interveiw_question.pdf', 'page': 1, 'score': 0.23088836669921875, 'preview': '🚀AIxFunda Newsletter                                                          aixfunda.substack.com                     ...'}, {'source': '../content/RAG_interveiw_question.pdf', 'page': 1, 'score': 0.23088836669921875, 'preview': '🚀AIxFunda Newsletter                                                          aixfunda.substack.com                     ...'}, {'source': '../content/RAG_interveiw_question.pdf', 'page': 1, 'score': 0.23088836669921875, 'preview': '🚀AIxFunda Newsletter                                                          aixfunda.substack.com                     ...'}], 'summary': \"Here's a 2-sentence summary:\\n\\nRAG (Retrieval-Augmented Generation) and LLM (Large Language Model) are techniques and models used to improve the accuracy of language models. RAG addresses the limitations of LLMs by retrieving relevant context from external knowledge sources, enabling them to provide accurate responses to queries related to the latest events or data not present in their training corpus.\"}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what is rag and llm and why they needed\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KF4UbYS_241n",
   "metadata": {
    "id": "KF4UbYS_241n"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04b27c5e69244ffab390520d47df6197": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_947fd7ad2de94df0933ae5f4bccbe001",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7de160539cec4a2c8b9244c5eba143ec",
      "value": 2
     }
    },
    "05a4e81e56a44e0aa6b2b0863183d54a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0a718cf4aee940e18de0fb5d04a8a829": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0df18fec09e14d358bea2efb20031d0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca3be090299048178f0b349cf38ba257",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90c8d432af6c4c569fae0351143db7cb",
      "value": 1
     }
    },
    "16110d55634644d6bec5a982af631ede": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "199dee77e9d5407fb87af36bdf5c86bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a277ddbf1994604b80bfdce965576be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1becf794a2b64c3e9ca7877643dd052b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9fa34ffae4c14310b328b51d603f7f89",
       "IPY_MODEL_51a949cbbca64f8693f68c3113ff37c0",
       "IPY_MODEL_71753c7643644ab7a6ec5e7cec8a2d9e"
      ],
      "layout": "IPY_MODEL_5b3f101cc22d402387f434cf9f4b9a39"
     }
    },
    "1c0c2f0da93c4fc292c880f594bab620": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d47cfb519ce45e89999975095df441b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "232df6642ca94a4fbb506b5fb965dc9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "28e199bf83274223b602a120e1f7a61b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44f9a69112754fbb8435efda6e2bb80c",
      "placeholder": "​",
      "style": "IPY_MODEL_59744726d85e4becad6fa705acc15806",
      "value": " 1/1 [00:00&lt;00:00, 24.71it/s]"
     }
    },
    "2cedf63aa58e497285c332251347fdf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3daa160a7b654e61b0fb9a771309c36d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec42626d450c4f5680b282f258ea2c0e",
      "value": 1
     }
    },
    "347dd82a33514a3a8400050f460d27fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39bccf75a3c64e79b8714d4f017300ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_421d29a438ef4c26959ca1888713090e",
       "IPY_MODEL_04b27c5e69244ffab390520d47df6197",
       "IPY_MODEL_fb48b38883e34365b2bf0954cee7ce6a"
      ],
      "layout": "IPY_MODEL_16110d55634644d6bec5a982af631ede"
     }
    },
    "3daa160a7b654e61b0fb9a771309c36d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "417549e4b2ea4abebe692599873f4f53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a277ddbf1994604b80bfdce965576be",
      "placeholder": "​",
      "style": "IPY_MODEL_347dd82a33514a3a8400050f460d27fd",
      "value": " 1/1 [00:00&lt;00:00, 20.87it/s]"
     }
    },
    "421d29a438ef4c26959ca1888713090e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55a33fddec374d70b5851615532e9890",
      "placeholder": "​",
      "style": "IPY_MODEL_ca8589213c6545819ed0713fc1b9b5eb",
      "value": "Batches: 100%"
     }
    },
    "432b3fa010174c1fb0872f30611cbd27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44f9a69112754fbb8435efda6e2bb80c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46412b2389434c08945cddeebcaaeda6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4651f67e2f504aae93fb76d5dfafb488": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51a949cbbca64f8693f68c3113ff37c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_658d2155560e459c9ec2fbbb3d1cf1a3",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6d19f709497433f9cd0603cb7984861",
      "value": 2
     }
    },
    "5379c43c344d43c5b388c2fb425613e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55a33fddec374d70b5851615532e9890": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59744726d85e4becad6fa705acc15806": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b3f101cc22d402387f434cf9f4b9a39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ba69c91d0694cabb5cbdda66fe52a1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_432b3fa010174c1fb0872f30611cbd27",
      "placeholder": "​",
      "style": "IPY_MODEL_81d58f99b601463aa29439a986c6cd29",
      "value": " 1/1 [00:00&lt;00:00, 23.16it/s]"
     }
    },
    "658d2155560e459c9ec2fbbb3d1cf1a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6788360aca254fcbae3fc5eb0004cdc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6adcc5a7b82943d098cf8457605005b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71753c7643644ab7a6ec5e7cec8a2d9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c612cf5ee17a47a9b80498a7a120ba67",
      "placeholder": "​",
      "style": "IPY_MODEL_9a3e96c7d45f469ab544d0d77cd2fa7f",
      "value": " 2/2 [00:05&lt;00:00,  2.27s/it]"
     }
    },
    "747207e0d44f4e3fb2b39754e60e2931": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75df689f9fde420794aa361f06b951cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7645cde6b10845a1a6afb97b9be99b26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7de160539cec4a2c8b9244c5eba143ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7f49d336e4064f89817613c6d9b839fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81d58f99b601463aa29439a986c6cd29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90c8d432af6c4c569fae0351143db7cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "947fd7ad2de94df0933ae5f4bccbe001": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a3e96c7d45f469ab544d0d77cd2fa7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d5de736341f41ddb3c3936fa6feec33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cb3e160e0fb141acb89cbb112541a4bd",
       "IPY_MODEL_a806c587c60f43acb7ff23ef844e68bd",
       "IPY_MODEL_5ba69c91d0694cabb5cbdda66fe52a1f"
      ],
      "layout": "IPY_MODEL_7645cde6b10845a1a6afb97b9be99b26"
     }
    },
    "9fa34ffae4c14310b328b51d603f7f89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a718cf4aee940e18de0fb5d04a8a829",
      "placeholder": "​",
      "style": "IPY_MODEL_232df6642ca94a4fbb506b5fb965dc9a",
      "value": "Batches: 100%"
     }
    },
    "a806c587c60f43acb7ff23ef844e68bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6adcc5a7b82943d098cf8457605005b7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f4bddee67a0d4a85a0f05680008b2a1f",
      "value": 1
     }
    },
    "b02233080b0a4850b1acd7f4146cc985": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba49aa7b95e546b99dc40f1a44ce564a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c14dec6946e644628dbc750341a911dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e1056f0b359f4ab5942dd27b132bf9c4",
       "IPY_MODEL_2cedf63aa58e497285c332251347fdf1",
       "IPY_MODEL_28e199bf83274223b602a120e1f7a61b"
      ],
      "layout": "IPY_MODEL_6788360aca254fcbae3fc5eb0004cdc6"
     }
    },
    "c612cf5ee17a47a9b80498a7a120ba67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c78f680bdc9b4bfb810cd96df3b991ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_747207e0d44f4e3fb2b39754e60e2931",
      "placeholder": "​",
      "style": "IPY_MODEL_7f49d336e4064f89817613c6d9b839fc",
      "value": "Batches: 100%"
     }
    },
    "ca3be090299048178f0b349cf38ba257": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca8589213c6545819ed0713fc1b9b5eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb3e160e0fb141acb89cbb112541a4bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b02233080b0a4850b1acd7f4146cc985",
      "placeholder": "​",
      "style": "IPY_MODEL_1d47cfb519ce45e89999975095df441b",
      "value": "Batches: 100%"
     }
    },
    "d2fba97f9a64455b947ca858a7e2184c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c78f680bdc9b4bfb810cd96df3b991ac",
       "IPY_MODEL_e643ede8d01d4de99526c3780b259452",
       "IPY_MODEL_417549e4b2ea4abebe692599873f4f53"
      ],
      "layout": "IPY_MODEL_75df689f9fde420794aa361f06b951cc"
     }
    },
    "d6215b5bd5204aa595a736cd89c7720d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df85d15c27d74ac5afd5f98afeff1a63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4aed730bd0d4560b8d5531c4bbd41c9",
      "placeholder": "​",
      "style": "IPY_MODEL_e6e6205d257c4da292a6a1c41d5c6d6d",
      "value": "Batches: 100%"
     }
    },
    "e1056f0b359f4ab5942dd27b132bf9c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4651f67e2f504aae93fb76d5dfafb488",
      "placeholder": "​",
      "style": "IPY_MODEL_46412b2389434c08945cddeebcaaeda6",
      "value": "Batches: 100%"
     }
    },
    "e643ede8d01d4de99526c3780b259452": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba49aa7b95e546b99dc40f1a44ce564a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_05a4e81e56a44e0aa6b2b0863183d54a",
      "value": 1
     }
    },
    "e6e6205d257c4da292a6a1c41d5c6d6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec42626d450c4f5680b282f258ea2c0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f4aed730bd0d4560b8d5531c4bbd41c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4bddee67a0d4a85a0f05680008b2a1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f6d19f709497433f9cd0603cb7984861": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f8b0904c3e264a708c65c74465972641": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fadfed920d6c479493c1ab1a07935b88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df85d15c27d74ac5afd5f98afeff1a63",
       "IPY_MODEL_0df18fec09e14d358bea2efb20031d0a",
       "IPY_MODEL_fc06f622e50a4223b587ebaa2b9147a7"
      ],
      "layout": "IPY_MODEL_d6215b5bd5204aa595a736cd89c7720d"
     }
    },
    "fb48b38883e34365b2bf0954cee7ce6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c0c2f0da93c4fc292c880f594bab620",
      "placeholder": "​",
      "style": "IPY_MODEL_199dee77e9d5407fb87af36bdf5c86bf",
      "value": " 2/2 [00:05&lt;00:00,  2.47s/it]"
     }
    },
    "fc06f622e50a4223b587ebaa2b9147a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5379c43c344d43c5b388c2fb425613e9",
      "placeholder": "​",
      "style": "IPY_MODEL_f8b0904c3e264a708c65c74465972641",
      "value": " 1/1 [00:00&lt;00:00, 21.82it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
